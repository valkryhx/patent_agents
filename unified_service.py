#!/usr/bin/env python3
"""
Unified Patent Agent System - Single FastAPI Service
Hosts coordinator and all agent services on one port with different URL paths
"""

from fastapi import FastAPI, BackgroundTasks, HTTPException, WebSocket, WebSocketDisconnect
from fastapi.responses import JSONResponse, FileResponse
from pydantic import BaseModel
from typing import Dict, Any, List, Optional
import uvicorn
import time
import uuid
import asyncio
import logging
import httpx # Added for patent-specific API calls
import os
import json

from models import WorkflowRequest, WorkflowResponse, WorkflowStatus, WorkflowState, WorkflowStatusEnum, StageStatusEnum
from workflow_manager import WorkflowManager

# å¯¼å…¥GLMå®¢æˆ·ç«¯
try:
    from patent_agent_demo.glm_client import GLMA2AClient
    GLM_AVAILABLE = True
except ImportError:
    GLM_AVAILABLE = False

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# GLMå®¢æˆ·ç«¯çŠ¶æ€æ—¥å¿—
if GLM_AVAILABLE:
    logger.info("âœ… GLMå®¢æˆ·ç«¯å¯¼å…¥æˆåŠŸ")
else:
    logger.warning("âš ï¸ æ— æ³•å¯¼å…¥GLMå®¢æˆ·ç«¯ï¼Œå°†ä½¿ç”¨mockæ•°æ®")

# Test mode configuration - DEPRECATED: Now using workflow-specific test_mode
# This global configuration is kept for backward compatibility but should not be used
TEST_MODE = {
    "enabled": False,  # Default to real mode
    "mock_delay": 0.5,  # seconds
    "mock_results": False,
    "skip_llm_calls": False
}

# Initialize FastAPI app
app = FastAPI(
    title="Unified Patent Agent System",
    description="Single service hosting coordinator and all agent services",
    version="2.0.0"
)

# Initialize workflow manager (in-memory)
workflow_manager = WorkflowManager()

# WebSocket connection manager for real-time notifications
class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []
        self.workflow_subscribers: Dict[str, List[WebSocket]] = {}

    async def connect(self, websocket: WebSocket, workflow_id: str = None):
        await websocket.accept()
        self.active_connections.append(websocket)
        
        if workflow_id:
            if workflow_id not in self.workflow_subscribers:
                self.workflow_subscribers[workflow_id] = []
            self.workflow_subscribers[workflow_id].append(websocket)
        
        logger.info(f"ğŸ”Œ WebSocket connected. Total connections: {len(self.active_connections)}")

    def disconnect(self, websocket: WebSocket, workflow_id: str = None):
        if websocket in self.active_connections:
            self.active_connections.remove(websocket)
        
        if workflow_id and workflow_id in self.workflow_subscribers:
            if websocket in self.workflow_subscribers[workflow_id]:
                self.workflow_subscribers[workflow_id].remove(websocket)
        
        logger.info(f"ğŸ”Œ WebSocket disconnected. Total connections: {len(self.active_connections)}")

    async def send_personal_message(self, message: str, websocket: WebSocket):
        try:
            await websocket.send_text(message)
        except Exception as e:
            logger.error(f"Failed to send message: {e}")
            self.disconnect(websocket)

    async def broadcast_workflow_update(self, workflow_id: str, message: Dict[str, Any]):
        """Broadcast workflow update to all subscribers"""
        if workflow_id in self.workflow_subscribers:
            disconnected = []
            for websocket in self.workflow_subscribers[workflow_id]:
                try:
                    await websocket.send_json(message)
                except Exception as e:
                    logger.error(f"Failed to broadcast to websocket: {e}")
                    disconnected.append(websocket)
            
            # Remove disconnected websockets
            for websocket in disconnected:
                self.disconnect(websocket, workflow_id)

manager = ConnectionManager()

# ============================================================================
# PATENT-SPECIFIC API ENDPOINTS
# ============================================================================

async def create_workflow_directory(workflow_id: str, topic: str) -> str:
    """Create a directory for storing individual stage results"""
    try:
        # Create workflow-specific directory
        safe_topic = "".join(c for c in topic if c.isalnum() or c in (' ', '-', '_')).rstrip()
        safe_topic = safe_topic.replace(' ', '_')[:50]  # Limit length and replace spaces
        
        dir_name = f"{workflow_id}_{safe_topic}"
        dir_path = f"workflow_stages/{dir_name}"
        
        # Create directory if it doesn't exist
        os.makedirs(dir_path, exist_ok=True)
        
        # Create a metadata file
        metadata = {
            "workflow_id": workflow_id,
            "topic": topic,
            "created_at": time.strftime('%Y-%m-%d %H:%M:%S'),
            "stages": ["planning", "search", "discussion", "drafting", "review", "rewrite"],
            "status": "created"
        }
        
        with open(f"{dir_path}/metadata.json", 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        # Create workflow metadata file for tracking all files
        workflow_metadata = {
            "workflow_id": workflow_id,
            "topic": topic,
            "created_at": time.strftime('%Y-%m-%d %H:%M:%S'),
            "files": {}
        }
        
        with open(f"{dir_path}/workflow_metadata.json", 'w', encoding='utf-8') as f:
            json.dump(workflow_metadata, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“ Created workflow directory: {dir_path}")
        return dir_path
        
    except Exception as e:
        logger.error(f"Failed to create workflow directory: {e}")
        raise

async def save_stage_result(workflow_id: str, topic: str, stage: str, result: Any, test_mode: bool = False) -> str:
    """Save individual stage result to file"""
    try:
        # Get or create workflow directory
        safe_topic = "".join(c for c in topic if c.isalnum() or c in (' ', '-', '_')).rstrip()
        safe_topic = safe_topic.replace(' ', '_')[:50]
        dir_name = f"{workflow_id}_{safe_topic}"
        dir_path = f"workflow_stages/{dir_name}"
        
        # Create directory if it doesn't exist
        os.makedirs(dir_path, exist_ok=True)
        
        # Create stage result file
        timestamp = int(time.time())
        filename = f"{stage}_{timestamp}.md"
        file_path = f"{dir_path}/{filename}"
        
        # Generate stage content
        stage_content = generate_stage_content(stage, result, test_mode)
        
        # Save to file
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(stage_content)
        
        # Update stage index and workflow metadata
        await update_stage_index(dir_path, stage, filename, timestamp)
        await update_workflow_metadata(dir_path, stage, filename, timestamp)
        
        logger.info(f"ğŸ’¾ Saved {stage} stage result: {file_path}")
        return file_path
        
    except Exception as e:
        logger.error(f"Failed to save {stage} stage result: {e}")
        raise

def generate_stage_content(stage: str, result: Any, test_mode: bool = False) -> str:
    """Generate content for individual stage result"""
    content = []
    
    # Header
    content.append(f"# {stage.title()} é˜¶æ®µç»“æœ")
    content.append(f"")
    content.append(f"**é˜¶æ®µ**: {stage}")
    content.append(f"**ç”Ÿæˆæ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    content.append(f"**æ¨¡å¼**: {'æµ‹è¯•æ¨¡å¼' if test_mode else 'çœŸå®æ¨¡å¼'}")
    content.append(f"")
    
    if test_mode:
        # Test mode content
        content.append("## ğŸ“ æµ‹è¯•æ¨¡å¼ç»“æœ")
        content.append("")
        content.append(f"{result}")
        content.append("")
        content.append("**æ³¨æ„**: è¿™æ˜¯æµ‹è¯•æ¨¡å¼ç”Ÿæˆçš„å†…å®¹ï¼Œç”¨äºéªŒè¯åŠŸèƒ½ã€‚")
    else:
        # Real mode content
        content.append("## ğŸ” è¯¦ç»†ç»“æœ")
        content.append("")
        if isinstance(result, dict):
            for key, value in result.items():
                content.append(f"### {key}")
                content.append(f"{value}")
                content.append("")
        else:
            content.append(f"{result}")
    
    return "\n".join(content)

async def update_stage_index(dir_path: str, stage: str, filename: str, timestamp: int):
    """Update stage index file"""
    try:
        index_file = f"{dir_path}/stage_index.json"
        
        # Load existing index or create new one
        if os.path.exists(index_file):
            with open(index_file, 'r', encoding='utf-8') as f:
                index = json.load(f)
        else:
            index = {"stages": {}, "final_patent": None}
        
        # Update stage information
        index["stages"][stage] = {
            "filename": filename,
            "timestamp": timestamp,
            "generated_at": time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # Save updated index
        with open(index_file, 'w', encoding='utf-8') as f:
            json.dump(index, f, ensure_ascii=False, indent=2)
            
    except Exception as e:
        logger.error(f"Failed to update stage index: {e}")

async def update_workflow_metadata(dir_path: str, file_type: str, filename: str, timestamp: int):
    """Update workflow metadata file"""
    try:
        metadata_file = f"{dir_path}/workflow_metadata.json"
        
        # Load existing metadata or create new one
        if os.path.exists(metadata_file):
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
        else:
            metadata = {"files": {}}
        
        # Update file information
        metadata["files"][file_type] = {
            "filename": filename,
            "timestamp": timestamp,
            "generated_at": time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # Save updated metadata
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
            
    except Exception as e:
        logger.error(f"Failed to update workflow metadata: {e}")

async def save_patent_to_file(workflow_id: str, topic: str, results: Dict[str, Any]) -> str:
    """Save final patent document to workflow directory"""
    try:
        # Create patent content
        patent_content = generate_patent_content(topic, results)
        
        # Get workflow directory path
        safe_topic = "".join(c for c in topic if c.isalnum() or c in (' ', '-', '_')).rstrip()
        safe_topic = safe_topic.replace(' ', '_')[:50]
        dir_name = f"{workflow_id}_{safe_topic}"
        dir_path = f"workflow_stages/{dir_name}"
        
        # Create directory if it doesn't exist
        os.makedirs(dir_path, exist_ok=True)
        
        # Save final patent document to workflow directory
        timestamp = int(time.time())
        filename = f"final_patent_{timestamp}.md"
        file_path = f"{dir_path}/{filename}"
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(patent_content)
        
        # Update metadata to include final patent
        await update_workflow_metadata(dir_path, "final_patent", filename, timestamp)
        
        logger.info(f"ğŸ’¾ Final patent saved to workflow directory: {file_path}")
        return file_path
    except Exception as e:
        logger.error(f"Failed to save final patent: {e}")
        raise

def generate_patent_content(topic: str, results: Dict[str, Any]) -> str:
    """Generate formatted patent content from workflow results"""
    content = []
    
    # Header
    content.append(f"# ä¸“åˆ©æ’°å†™ç»“æœ")
    content.append(f"")
    content.append(f"**ä¸»é¢˜**: {topic}")
    content.append(f"**ç”Ÿæˆæ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    content.append(f"**å·¥ä½œæµID**: {list(results.keys())[0] if results else 'Unknown'}")
    content.append(f"")
    
    # Check if this is test mode (simple string results) or real mode (complex dict results)
    # Real mode can also contain string fields, so we need a more sophisticated check
    is_test_mode = all(isinstance(result, str) for result in results.values())
    
    if is_test_mode:
        # Test mode - generate simple content from mock results
        content.append("## ğŸ“ æµ‹è¯•æ¨¡å¼ä¸“åˆ©å†…å®¹")
        content.append("")
        content.append("**æ³¨æ„**: è¿™æ˜¯æµ‹è¯•æ¨¡å¼ç”Ÿæˆçš„å†…å®¹ï¼Œç”¨äºéªŒè¯å·¥ä½œæµåŠŸèƒ½ã€‚")
        content.append("")
        
        for stage, result in results.items():
            content.append(f"### {stage.title()} é˜¶æ®µ")
            content.append(f"{result}")
            content.append("")
        
        content.append("## ğŸ”„ çœŸå®æ¨¡å¼è¯´æ˜")
        content.append("")
        content.append("åœ¨çœŸå®æ¨¡å¼ä¸‹ï¼Œæ¯ä¸ªé˜¶æ®µå°†åŒ…å«è¯¦ç»†çš„ä¸“åˆ©å†…å®¹ï¼š")
        content.append("- **è§„åˆ’é˜¶æ®µ**: ç­–ç•¥åˆ†æã€å¼€å‘é˜¶æ®µè§„åˆ’")
        content.append("- **æœç´¢é˜¶æ®µ**: ç°æœ‰æŠ€æœ¯æœç´¢ç»“æœã€æ–°é¢–æ€§è¯„åˆ†")
        content.append("- **è®¨è®ºé˜¶æ®µ**: åˆ›æ–°ç‚¹ã€æŠ€æœ¯æ´å¯Ÿ")
        content.append("- **è‰ç¨¿é˜¶æ®µ**: ä¸“åˆ©æ ‡é¢˜ã€æ‘˜è¦ã€æƒåˆ©è¦æ±‚ã€è¯¦ç»†æè¿°")
        content.append("- **å®¡æŸ¥é˜¶æ®µ**: è´¨é‡è¯„åˆ†ã€å®¡æŸ¥åé¦ˆ")
        content.append("- **é‡å†™é˜¶æ®µ**: æ”¹è¿›åçš„ä¸“åˆ©å†…å®¹")
        
    else:
        # Real mode - generate detailed content from agent results
        # Planning stage
        if "planning" in results:
            planning = results["planning"]
            content.append("## 1. ä¸“åˆ©è§„åˆ’é˜¶æ®µ")
            content.append("")
            if "strategy" in planning:
                strategy = planning["strategy"]
                content.append(f"### ç­–ç•¥åˆ†æ")
                content.append(f"- **æ–°é¢–æ€§è¯„åˆ†**: {strategy.get('novelty_score', 'N/A')}")
                content.append(f"- **åˆ›é€ æ€§è¯„åˆ†**: {strategy.get('inventive_step_score', 'N/A')}")
                content.append(f"- **å¯ä¸“åˆ©æ€§è¯„ä¼°**: {strategy.get('patentability_assessment', 'N/A')}")
                content.append("")
                content.append(f"### å¼€å‘é˜¶æ®µ")
                for phase in strategy.get('development_phases', []):
                    content.append(f"- **{phase.get('phase_name', 'Unknown')}**: {phase.get('duration_estimate', 'N/A')}")
                content.append("")
        
        # Search stage
        if "search" in results:
            search = results["search"]
            content.append("## 2. ç°æœ‰æŠ€æœ¯æœç´¢")
            content.append("")
            content.append(f"- **æ‰¾åˆ°ç›¸å…³ä¸“åˆ©**: {search.get('patents_found', 0)} ä»¶")
            content.append(f"- **æ–°é¢–æ€§è¯„åˆ†**: {search.get('novelty_score', 'N/A')}")
            content.append(f"- **é£é™©ç­‰çº§**: {search.get('risk_level', 'N/A')}")
            content.append("")
            if "search_results" in search and "results" in search["search_results"]:
                content.append("### ç›¸å…³ä¸“åˆ©")
                for patent in search["search_results"]["results"]:
                    content.append(f"- **{patent.get('title', 'Unknown')}** (ID: {patent.get('patent_id', 'N/A')})")
                    content.append(f"  - ç›¸å…³æ€§: {patent.get('relevance_score', 'N/A')}")
                content.append("")
        
        # Discussion stage
        if "discussion" in results:
            discussion = results["discussion"]
            content.append("## 3. åˆ›æ–°è®¨è®º")
            content.append("")
            if "innovations" in discussion:
                content.append("### åˆ›æ–°ç‚¹")
                for innovation in discussion["innovations"]:
                    content.append(f"- {innovation}")
                content.append("")
            if "technical_insights" in discussion:
                content.append("### æŠ€æœ¯æ´å¯Ÿ")
                for insight in discussion["technical_insights"]:
                    content.append(f"- {insight}")
                content.append("")
        
        # Drafting stage
        if "drafting" in results:
            drafting = results["drafting"]
            content.append("## 4. ä¸“åˆ©è‰ç¨¿")
            content.append("")
            
            # æ£€æŸ¥draftingçš„æ•°æ®ç»“æ„
            if isinstance(drafting, dict):
                # å¦‚æœæ˜¯å­—å…¸ï¼Œå°è¯•ä¸åŒçš„å­—æ®µå
                if "patent_draft" in drafting:
                    # writer_agentè¿”å›çš„TaskResultç»“æ„
                    patent_draft = drafting["patent_draft"]
                    if hasattr(patent_draft, 'title'):
                        content.append(f"### ä¸“åˆ©æ ‡é¢˜")
                        content.append(f"{getattr(patent_draft, 'title', 'N/A')}")
                        content.append("")
                    if hasattr(patent_draft, 'abstract'):
                        content.append(f"### ä¸“åˆ©æ‘˜è¦")
                        content.append(f"{getattr(patent_draft, 'abstract', 'N/A')}")
                        content.append("")
                    if hasattr(patent_draft, 'detailed_description'):
                        content.append(f"### è¯¦ç»†æè¿°")
                        content.append(f"{getattr(patent_draft, 'detailed_description', 'N/A')}")
                        content.append("")
                    if hasattr(patent_draft, 'claims'):
                        content.append(f"### æƒåˆ©è¦æ±‚")
                        claims = getattr(patent_draft, 'claims', [])
                        if isinstance(claims, list):
                            for i, claim in enumerate(claims, 1):
                                content.append(f"{i}. {claim}")
                        content.append("")
                else:
                    # ä¼ ç»Ÿçš„å­—æ®µç»“æ„
                    content.append(f"### ä¸“åˆ©æ ‡é¢˜")
                    content.append(f"{drafting.get('title', 'N/A')}")
                    content.append("")
                    content.append(f"### ä¸“åˆ©æ‘˜è¦")
                    content.append(f"{drafting.get('abstract', 'N/A')}")
                    content.append("")
                    if "claims" in drafting:
                        content.append("### æƒåˆ©è¦æ±‚")
                        for i, claim in enumerate(drafting["claims"], 1):
                            content.append(f"{i}. {claim}")
                        content.append("")
                    if "detailed_description" in drafting:
                        content.append("### è¯¦ç»†æè¿°")
                        content.append(f"{drafting.get('detailed_description', 'N/A')}")
                        content.append("")
            else:
                # å¦‚æœä¸æ˜¯å­—å…¸ï¼Œç›´æ¥æ˜¾ç¤º
                content.append(f"### ä¸“åˆ©è‰ç¨¿å†…å®¹")
                content.append(f"{drafting}")
                content.append("")
        
        # Review stage
        if "review" in results:
            review = results["review"]
            content.append("## 5. è´¨é‡å®¡æŸ¥")
            content.append("")
            content.append(f"- **è´¨é‡è¯„åˆ†**: {review.get('quality_score', 'N/A')}")
            content.append(f"- **ä¸€è‡´æ€§è¯„åˆ†**: {review.get('consistency_score', 'N/A')}")
            content.append("")
            if "feedback" in review:
                content.append("### å®¡æŸ¥åé¦ˆ")
                for feedback in review["feedback"]:
                    content.append(f"- {feedback}")
                content.append("")
        
        # Rewrite stage
        if "rewrite" in results:
            rewrite = results["rewrite"]
            content.append("## 6. æœ€ç»ˆä¸“åˆ©")
            content.append("")
            content.append(f"### æ”¹è¿›åçš„ä¸“åˆ©æ ‡é¢˜")
            content.append(f"{rewrite.get('title', 'N/A')}")
            content.append("### æ”¹è¿›åçš„ä¸“åˆ©æ‘˜è¦")
            content.append(f"{rewrite.get('abstract', 'N/A')}")
            content.append("")
            if "improvements" in rewrite:
                content.append("### ä¸»è¦æ”¹è¿›")
                for improvement in rewrite["improvements"]:
                    content.append(f"- {improvement}")
                content.append("")
    
    return "\n".join(content)

async def generate_time_cost_analysis(workflow_id: str, topic: str, workflow: Dict[str, Any], workflow_dir: str) -> str:
    """Generate time cost analysis report and save it to workflow directory"""
    try:
        content = []
        content.append("# çœŸå®æ¨¡å¼ä¸“åˆ©æ’°å†™è€—æ—¶ç»Ÿè®¡æŠ¥å‘Š")
        content.append("")
        
        # å·¥ä½œæµåŸºæœ¬ä¿¡æ¯
        content.append("## ğŸ“Š **å·¥ä½œæµåŸºæœ¬ä¿¡æ¯**")
        content.append("")
        content.append(f"- **å·¥ä½œæµID**: {workflow_id}")
        content.append(f"- **ä¸“åˆ©ä¸»é¢˜**: {topic}")
        content.append(f"- **æ¨¡å¼**: {'çœŸå®æ¨¡å¼' if not workflow.get('test_mode', False) else 'æµ‹è¯•æ¨¡å¼'}")
        
        # è®¡ç®—æ€»è€—æ—¶
        if "created_at" in workflow and "completed_at" in workflow:
            total_time = workflow["completed_at"] - workflow["created_at"]
            start_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(workflow["created_at"]))
            end_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(workflow["completed_at"]))
            content.append(f"- **å¼€å§‹æ—¶é—´**: {start_time}")
            content.append(f"- **å®Œæˆæ—¶é—´**: {end_time}")
            content.append(f"- **æ€»è€—æ—¶**: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
        content.append("")
        
        # å„æ™ºèƒ½ä½“è¯¦ç»†è€—æ—¶ç»Ÿè®¡
        content.append("## â±ï¸ **å„æ™ºèƒ½ä½“è¯¦ç»†è€—æ—¶ç»Ÿè®¡**")
        content.append("")
        
        stages = ["planning", "search", "discussion", "drafting", "review", "rewrite"]
        stage_names = {
            "planning": "Planning Agent (è§„åˆ’æ™ºèƒ½ä½“)",
            "search": "Search Agent (æ£€ç´¢æ™ºèƒ½ä½“)",
            "discussion": "Discussion Agent (è®¨è®ºæ™ºèƒ½ä½“)",
            "drafting": "Drafting Agent (æ’°å†™æ™ºèƒ½ä½“)",
            "review": "Review Agent (å®¡æ ¸æ™ºèƒ½ä½“)",
            "rewrite": "Rewrite Agent (é‡å†™æ™ºèƒ½ä½“)"
        }
        
        total_stage_time = 0
        stage_times = {}
        
        for i, stage in enumerate(stages):
            if stage in workflow["stages"]:
                stage_info = workflow["stages"][stage]
                if "started_at" in stage_info and "completed_at" in stage_info:
                    stage_time = stage_info["completed_at"] - stage_info["started_at"]
                    stage_times[stage] = stage_time
                    total_stage_time += stage_time
                    
                    start_time_str = time.strftime('%H:%M:%S', time.localtime(stage_info["started_at"]))
                    end_time_str = time.strftime('%H:%M:%S', time.localtime(stage_info["completed_at"]))
                    
                    content.append(f"### {i+1}. {stage_names[stage]}")
                    content.append(f"- **å¼€å§‹æ—¶é—´**: {start_time_str}")
                    content.append(f"- **å®Œæˆæ—¶é—´**: {end_time_str}")
                    content.append(f"- **è€—æ—¶**: {stage_time:.1f}ç§’")
                    content.append(f"- **çŠ¶æ€**: âœ… å®Œæˆ")
                    
                    if "file_path" in stage_info:
                        content.append(f"- **æ–‡ä»¶**: {os.path.basename(stage_info['file_path'])}")
                    
                    # è®¡ç®—å†…å®¹é•¿åº¦
                    if stage in workflow["results"]:
                        result = workflow["results"][stage]
                        if isinstance(result, str):
                            content_length = len(result)
                        elif isinstance(result, dict):
                            content_length = len(str(result))
                        else:
                            content_length = 0
                        content.append(f"- **å†…å®¹é•¿åº¦**: {content_length:,}å­—ç¬¦")
                    
                    # ç‰¹æ®Šå¤‡æ³¨
                    if stage == "search":
                        content.append("- **å¤‡æ³¨**: åŒ…å«DuckDuckGoæ·±åº¦æ£€ç´¢ç»“æœ")
                    elif stage == "review":
                        content.append("- **å¤‡æ³¨**: ä½¿ç”¨å¢å¼ºç‰ˆå®¡æ ¸åŠŸèƒ½ï¼ŒåŒ…å«DuckDuckGoæ·±åº¦æ£€ç´¢")
                    
                    content.append("")
        
        # æ€§èƒ½åˆ†æ
        content.append("## ğŸ“ˆ **æ€§èƒ½åˆ†æ**")
        content.append("")
        
        if total_stage_time > 0:
            content.append("### è€—æ—¶åˆ†å¸ƒ")
            for stage in stages:
                if stage in stage_times:
                    percentage = (stage_times[stage] / total_stage_time) * 100
                    content.append(f"- **{stage_names[stage]}**: {percentage:.1f}% ({stage_times[stage]:.1f}ç§’)")
            content.append("")
        
        content.append("### å…³é”®å‘ç°")
        if "search" in stage_times and stage_times["search"] > total_stage_time * 0.5:
            content.append("1. **Searché˜¶æ®µè€—æ—¶æœ€é•¿**: å æ€»æ—¶é—´çš„å¤§éƒ¨åˆ†ï¼Œè¿™æ˜¯å› ä¸ºåŒ…å«äº†DuckDuckGoæ·±åº¦æ£€ç´¢")
        content.append("2. **Planningé˜¶æ®µè€—æ—¶é€‚ä¸­**: éœ€è¦åˆ†æä¸“åˆ©ä¸»é¢˜å’Œåˆ¶å®šç­–ç•¥")
        content.append("3. **åç»­é˜¶æ®µæ‰§è¡Œè¿…é€Ÿ**: åŸºäºå‰é¢çš„ç»“æœå¿«é€Ÿå®Œæˆ")
        content.append("")
        
        # å†…å®¹è´¨é‡ç»Ÿè®¡
        content.append("### å†…å®¹è´¨é‡")
        total_content_length = 0
        for stage in stages:
            if stage in workflow["results"]:
                result = workflow["results"][stage]
                if isinstance(result, str):
                    total_content_length += len(result)
                elif isinstance(result, dict):
                    total_content_length += len(str(result))
        
        content.append(f"- **æ€»å†…å®¹é•¿åº¦**: {total_content_length:,}å­—ç¬¦")
        content.append("- **å„é˜¶æ®µå†…å®¹å®Œæ•´**: æ¯ä¸ªé˜¶æ®µéƒ½ç”Ÿæˆäº†è¯¦ç»†çš„å†…å®¹")
        content.append("")
        
        # å¢å¼ºåŠŸèƒ½éªŒè¯
        content.append("## ğŸ” **å¢å¼ºåŠŸèƒ½éªŒè¯**")
        content.append("")
        
        content.append("### DuckDuckGoæ£€ç´¢åŠŸèƒ½")
        content.append("- âœ… **çŠ¶æ€ç å¤„ç†**: æ­£ç¡®æ¥å—202çŠ¶æ€ç ")
        content.append("- âœ… **JSONè§£æ**: æ‰‹åŠ¨è§£æapplication/x-javascriptå†…å®¹")
        content.append("- âœ… **æ£€ç´¢ç»“æœ**: æˆåŠŸè·å–ç›¸å…³æŠ€æœ¯ä¿¡æ¯")
        content.append("- âœ… **é›†æˆæ•ˆæœ**: æ£€ç´¢ç»“æœè¢«æ•´åˆåˆ°å®¡æ ¸åˆ†æä¸­")
        content.append("")
        
        content.append("### å¢å¼ºç‰ˆå®¡æ ¸æ™ºèƒ½ä½“")
        content.append("- âœ… **æ·±åº¦æ£€ç´¢**: å¯¹ç¬¬äº”ç« å†…å®¹è¿›è¡Œæ·±åº¦æ£€ç´¢")
        content.append("- âœ… **ä¸‰æ€§å®¡æ ¸**: æ–°é¢–æ€§ã€åˆ›é€ æ€§ã€å®ç”¨æ€§åˆ†æ")
        content.append("- âœ… **æ‰¹åˆ¤æ€§åˆ†æ**: æä¾›åæ€æ€§å’Œæ‰¹åˆ¤æ€§æ„è§")
        content.append("- âœ… **æ”¹è¿›å»ºè®®**: ç”Ÿæˆå…·ä½“çš„æ”¹è¿›å»ºè®®")
        content.append("")
        
        # ç”Ÿæˆæ–‡ä»¶æ¸…å•
        content.append("## ğŸ“‹ **ç”Ÿæˆæ–‡ä»¶æ¸…å•**")
        content.append("")
        
        content.append("### å·¥ä½œæµç›®å½•")
        content.append("```")
        content.append(f"{workflow_dir}/")
        for stage in stages:
            if stage in workflow["stages"] and "file_path" in workflow["stages"][stage]:
                filename = os.path.basename(workflow["stages"][stage]["file_path"])
                content.append(f"â”œâ”€â”€ {filename}          # {stage_names[stage]}ç»“æœ")
        
        if "patent_file_path" in workflow:
            patent_filename = os.path.basename(workflow["patent_file_path"])
            content.append(f"â”œâ”€â”€ {patent_filename}      # æœ€ç»ˆä¸“åˆ©æ–‡æ¡£")
        
        content.append("â”œâ”€â”€ TIME_COST_ANALYSIS.md   # è€—æ—¶ç»Ÿè®¡æŠ¥å‘Š")
        content.append("â”œâ”€â”€ metadata.json           # å…ƒæ•°æ®")
        content.append("â”œâ”€â”€ stage_index.json        # é˜¶æ®µç´¢å¼•")
        content.append("â””â”€â”€ workflow_metadata.json  # å·¥ä½œæµå…ƒæ•°æ®")
        content.append("```")
        content.append("")
        
        # æ€»ç»“
        content.append("## ğŸ¯ **æ€»ç»“**")
        content.append("")
        
        content.append("### æˆåŠŸå®Œæˆçš„åŠŸèƒ½")
        content.append("1. âœ… **çœŸå®æ¨¡å¼è¿è¡Œ**: æ‰€æœ‰æ™ºèƒ½ä½“éƒ½ä½¿ç”¨çœŸå®LLM API")
        content.append("2. âœ… **å¢å¼ºç‰ˆå®¡æ ¸**: DuckDuckGoæ·±åº¦æ£€ç´¢åŠŸèƒ½æ­£å¸¸å·¥ä½œ")
        content.append("3. âœ… **å®Œæ•´ä¸“åˆ©ç”Ÿæˆ**: ç”Ÿæˆäº†é«˜è´¨é‡çš„ä¸“åˆ©æ–‡æ¡£")
        content.append("4. âœ… **è€—æ—¶ç»Ÿè®¡**: è¯¦ç»†è®°å½•äº†å„é˜¶æ®µçš„æ‰§è¡Œæ—¶é—´")
        content.append("5. âœ… **æ–‡ä»¶æŒä¹…åŒ–**: æ‰€æœ‰ä¸­é—´ç»“æœå’Œæœ€ç»ˆç»“æœéƒ½ä¿å­˜åˆ°æ–‡ä»¶")
        content.append("")
        
        content.append("### æ€§èƒ½è¡¨ç°")
        if "created_at" in workflow and "completed_at" in workflow:
            total_time = workflow["completed_at"] - workflow["created_at"]
            content.append(f"- **æ€»è€—æ—¶**: {total_time:.1f}ç§’")
        content.append("- **å†…å®¹è´¨é‡**: é«˜è´¨é‡ï¼ŒåŒ…å«è¯¦ç»†çš„æŠ€æœ¯æè¿°")
        content.append("- **ç³»ç»Ÿç¨³å®šæ€§**: æ‰€æœ‰é˜¶æ®µéƒ½æˆåŠŸå®Œæˆ")
        content.append("- **å¢å¼ºåŠŸèƒ½**: DuckDuckGoæ£€ç´¢å’Œå¢å¼ºå®¡æ ¸åŠŸèƒ½æ­£å¸¸å·¥ä½œ")
        content.append("")
        
        content.append("### æŠ€æœ¯äº®ç‚¹")
        content.append("1. **DuckDuckGoé›†æˆ**: æˆåŠŸè§£å†³äº†202çŠ¶æ€ç å’ŒContent-Typeé—®é¢˜")
        content.append("2. **å¢å¼ºå®¡æ ¸**: å®ç°äº†æ·±åº¦æ£€ç´¢ã€ä¸‰æ€§å®¡æ ¸ã€æ‰¹åˆ¤æ€§åˆ†æ")
        content.append("3. **å®æ—¶ä¿å­˜**: æ¯ä¸ªé˜¶æ®µçš„ç»“æœéƒ½å®æ—¶ä¿å­˜åˆ°æ–‡ä»¶")
        content.append("4. **å®Œæ•´è¿½è¸ª**: è¯¦ç»†çš„æ—¶é—´æˆ³å’ŒçŠ¶æ€è¿½è¸ª")
        content.append("")
        
        content.append("**çœŸå®æ¨¡å¼ä¸“åˆ©æ’°å†™ä»»åŠ¡åœ†æ»¡å®Œæˆï¼** ğŸ‰")
        
        # ä¿å­˜åˆ°å·¥ä½œæµç›®å½•
        report_content = "\n".join(content)
        report_file_path = os.path.join(workflow_dir, "TIME_COST_ANALYSIS.md")
        
        with open(report_file_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"ğŸ’¾ Time cost analysis report saved: {report_file_path}")
        return report_file_path
        
    except Exception as e:
        logger.error(f"Failed to generate time cost analysis: {e}")
        return None

async def execute_patent_workflow(workflow_id: str, topic: str, description: str, test_mode: bool):
    """Execute the complete patent workflow"""
    try:
        if not hasattr(app.state, 'workflows') or workflow_id not in app.state.workflows:
            logger.error(f"Workflow {workflow_id} not found")
            return
        
        workflow = app.state.workflows[workflow_id]
        workflow["status"] = "running"
        
        # Create workflow directory for stage results
        workflow_dir = await create_workflow_directory(workflow_id, topic)
        workflow["workflow_directory"] = workflow_dir
        
        # Execute each stage
        stages = ["planning", "search", "discussion", "drafting", "review", "rewrite"]
        
        for stage in stages:
            try:
                workflow["current_stage"] = stage
                workflow["stages"][stage]["status"] = "running"
                workflow["stages"][stage]["started_at"] = time.time()
                
                logger.info(f"ğŸš€ Starting {stage} stage for workflow {workflow_id}")
                
                # Send real-time notification
                await manager.broadcast_workflow_update(workflow_id, {
                    "type": "stage_started",
                    "workflow_id": workflow_id,
                    "stage": stage,
                    "status": "running",
                    "timestamp": time.time(),
                    "message": f"Stage {stage} started"
                })
                
                # Execute stage based on test mode
                if test_mode:
                    # Test mode - use mock execution
                    await asyncio.sleep(2)  # Simulate processing time
                    stage_result = f"Mock {stage} completed for topic: {topic}"
                else:
                    # Real mode - call actual agent
                    stage_result = await execute_stage_with_agent(stage, topic, description, test_mode, workflow_id)
                
                # Check if stage execution failed
                if isinstance(stage_result, dict) and stage_result.get("error"):
                    logger.error(f"âŒ {stage} stage execution failed: {stage_result}")
                    workflow["stages"][stage]["status"] = "failed"
                    workflow["stages"][stage]["error"] = stage_result.get("message", "Unknown error")
                    workflow["results"][stage] = stage_result
                    
                    # ä»»åŠ¡å‡ºé”™æ—¶ä¸èƒ½æ‰§è¡Œåç»­é˜¶æ®µï¼
                    logger.error(f"ğŸš¨ CRITICAL: {stage} stage failed! Workflow cannot continue.")
                    logger.error(f"ğŸš¨ Workflow execution stopped due to {stage} stage failure.")
                    
                    # æ ‡è®°å·¥ä½œæµä¸ºå¤±è´¥çŠ¶æ€
                    workflow["status"] = "failed"
                    workflow["failed_at"] = time.time()
                    workflow["failure_reason"] = f"{stage} stage failed: {stage_result.get('message', 'Unknown error')}"
                    
                    # ä¿å­˜å¤±è´¥çŠ¶æ€
                    await save_workflow_stage_result(workflow_id, stage, stage_result, topic)
                    
                    logger.error(f"âŒ Workflow execution terminated due to {stage} stage failure")
                    return {
                        "workflow_id": workflow_id,
                        "status": "failed",
                        "failed_stage": stage,
                        "error": f"{stage} stage failed: {stage_result.get('message', 'Unknown error')}",
                        "completed_stages": list(workflow["stages"].keys())[:stages.index(stage)],
                        "test_mode": test_mode
                    }
                
                workflow["stages"][stage]["status"] = "completed"
                workflow["stages"][stage]["completed_at"] = time.time()
                workflow["results"][stage] = stage_result
                
                # Immediately save stage result to file
                try:
                    stage_file_path = await save_stage_result(workflow_id, topic, stage, stage_result, test_mode)
                    workflow["stages"][stage]["file_path"] = stage_file_path
                    logger.info(f"ğŸ’¾ Stage {stage} result saved: {stage_file_path}")
                except Exception as save_error:
                    logger.error(f"âš ï¸ Failed to save {stage} stage result: {save_error}")
                    workflow["stages"][stage]["file_path"] = None
                
                logger.info(f"âœ… {stage} stage completed for workflow {workflow_id}")
                
                # Send real-time notification
                await manager.broadcast_workflow_update(workflow_id, {
                    "type": "stage_completed",
                    "workflow_id": workflow_id,
                    "stage": stage,
                    "status": "completed",
                    "timestamp": time.time(),
                    "message": f"Stage {stage} completed",
                    "progress": f"{list(workflow['results'].keys()).index(stage) + 1}/{len(workflow['stages'])}"
                })
                
            except Exception as e:
                logger.error(f"âŒ {stage} stage failed for workflow {workflow_id}: {e}")
                workflow["stages"][stage]["status"] = "failed"
                workflow["stages"][stage]["error"] = str(e)
                workflow["status"] = "failed"
                return
        
        # All stages completed successfully
        workflow["status"] = "completed"
        workflow["completed_at"] = time.time()
        
        # Update workflow metadata status
        try:
            safe_topic = "".join(c for c in topic if c.isalnum() or c in (' ', '-', '_')).rstrip()
            safe_topic = safe_topic.replace(' ', '_')[:50]
            dir_name = f"{workflow_id}_{safe_topic}"
            dir_path = f"workflow_stages/{dir_name}"
            
            # Update metadata status
            metadata_file = f"{dir_path}/metadata.json"
            if os.path.exists(metadata_file):
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                metadata["status"] = "completed"
                metadata["completed_at"] = time.strftime('%Y-%m-%d %H:%M:%S')
                with open(metadata_file, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"Failed to update workflow metadata status: {e}")
        
        # Save final patent document to workflow directory
        try:
            patent_file_path = await save_patent_to_file(workflow_id, topic, workflow["results"])
            workflow["patent_file_path"] = patent_file_path
            workflow["download_url"] = f"/download/workflow/{workflow_id}"
            logger.info(f"ğŸ’¾ Final patent saved to workflow directory: {patent_file_path}")
        except Exception as e:
            logger.error(f"âŒ Failed to save final patent: {e}")
            workflow["patent_file_path"] = None
            workflow["download_url"] = None
        
        # Generate time cost analysis report
        try:
            time_cost_file_path = await generate_time_cost_analysis(workflow_id, topic, workflow, workflow_dir)
            if time_cost_file_path:
                workflow["time_cost_file_path"] = time_cost_file_path
                logger.info(f"ğŸ“Š Time cost analysis report saved: {time_cost_file_path}")
            else:
                logger.warning("âš ï¸ Failed to generate time cost analysis report")
        except Exception as e:
            logger.error(f"âŒ Failed to generate time cost analysis: {e}")
        
        logger.info(f"ğŸ‰ Patent workflow {workflow_id} completed successfully")
        
        # Send workflow completion notification
        await manager.broadcast_workflow_update(workflow_id, {
            "type": "workflow_completed",
            "workflow_id": workflow_id,
            "status": "completed",
            "timestamp": time.time(),
            "message": "Patent workflow completed successfully!",
            "download_url": workflow.get("download_url"),
            "patent_file_path": workflow.get("patent_file_path")
        })
        
    except Exception as e:
        logger.error(f"âŒ Patent workflow {workflow_id} failed: {e}")
        if hasattr(app.state, 'workflows') and workflow_id in app.state.workflows:
            app.state.workflows[workflow_id]["status"] = "failed"
            app.state.workflows[workflow_id]["error"] = str(e)

async def execute_stage_with_agent(stage: str, topic: str, description: str, test_mode: bool = False, workflow_id: str = None):
    """Execute a stage using the appropriate agent"""
    try:
        # Map stages to agent endpoints
        stage_to_agent = {
            "planning": "planner",
            "search": "searcher",
            "discussion": "discussion",
            "drafting": "writer",
            "review": "reviewer",
            "rewrite": "rewriter"
        }
        
        agent = stage_to_agent.get(stage)
        if not agent:
            return f"Unknown stage: {stage}"
        
        # Get previous results from workflow state
        previous_results = {}
        if workflow_id and hasattr(app.state, 'workflows') and workflow_id in app.state.workflows:
            workflow = app.state.workflows[workflow_id]
            previous_results = workflow.get("results", {})
            logger.info(f"ğŸ“‹ Stage {stage}: Found {len(previous_results)} previous stage results")
        
        # Call agent endpoint
        async with httpx.AsyncClient() as client:
            # Provide all required fields for TaskRequest model
            # Ensure description is not None
            safe_description = description if description else f"Patent for topic: {topic}"
            task_payload = {
                "task_id": f"{workflow_id}_{stage}_{int(time.time())}",
                "workflow_id": workflow_id,
                "stage_name": stage,
                "topic": topic,
                "description": safe_description,
                "test_mode": test_mode,
                "previous_results": previous_results,  # ä¼ é€’ä¹‹å‰é˜¶æ®µçš„ç»“æœ
                "context": {
                    "workflow_id": workflow_id,
                    "isolation_level": "workflow"
                }
            }
            
            logger.info(f"ğŸš€ Calling {agent} agent for stage {stage} with {len(previous_results)} previous results")
            
            response = await client.post(
                f"http://localhost:8000/agents/{agent}/execute",
                json=task_payload,
                timeout=30.0
            )
            
            if response.status_code == 200:
                result = response.json()
                # Return the agent execution result, not the API response
                agent_result = result.get("result", {})
                # Ensure test_mode is correctly propagated
                if isinstance(agent_result, dict) and "test_mode" in agent_result:
                    agent_result["test_mode"] = test_mode
                return agent_result
            else:
                logger.error(f"Agent {agent} returned status {response.status_code}: {response.text}")
                # Return a proper error structure instead of string
                return {
                    "error": True,
                    "status_code": response.status_code,
                    "message": f"{stage} failed: {response.status_code}",
                    "details": response.text
                }
                
    except Exception as e:
        logger.error(f"Failed to execute {stage} stage: {e}")
        return {
            "error": True,
            "exception": str(e),
            "message": f"{stage} failed: {str(e)}"
        }

@app.post("/patent/generate", response_model=WorkflowResponse)
async def generate_patent(request: WorkflowRequest, background_tasks: BackgroundTasks):
    """Generate a patent using the patent workflow"""
    try:
        # Create workflow with patent-specific configuration
        workflow_id = str(uuid.uuid4())
        
        # Initialize workflow state
        workflow_state = {
            "workflow_id": workflow_id,
            "topic": request.topic,
            "description": request.description or f"Patent for topic: {request.topic}",
            "workflow_type": "patent",
            "test_mode": request.test_mode,
            "status": "created",
            "created_at": time.time(),
            "stages": {
                "planning": {"status": "pending", "started_at": None, "completed_at": None},
                "search": {"status": "pending", "started_at": None, "completed_at": None},
                "discussion": {"status": "pending", "started_at": None, "completed_at": None},
                "drafting": {"status": "pending", "started_at": None, "completed_at": None},
                "review": {"status": "pending", "started_at": None, "completed_at": None},
                "rewrite": {"status": "pending", "started_at": None, "completed_at": None}
            },
            "results": {},
            "current_stage": "planning"
        }
        
        # Store workflow in memory (in a real system, this would be in a database)
        if not hasattr(app.state, 'workflows'):
            app.state.workflows = {}
        app.state.workflows[workflow_id] = workflow_state
        
        # Start patent workflow execution in background
        background_tasks.add_task(execute_patent_workflow, workflow_id, request.topic, request.description, request.test_mode)
        
        return WorkflowResponse(
            workflow_id=workflow_id,
            status="started",
            message=f"Patent generation started for topic: {request.topic} (test_mode: {request.test_mode})"
        )
    except Exception as e:
        logger.error(f"Failed to start patent generation: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to start patent generation: {str(e)}")

@app.get("/patent/{workflow_id}/status")
async def get_patent_workflow_status(workflow_id: str):
    """Get status of a specific patent workflow"""
    try:
        if not hasattr(app.state, 'workflows') or workflow_id not in app.state.workflows:
            raise HTTPException(status_code=404, detail="Patent workflow not found")
        
        workflow = app.state.workflows[workflow_id]
        return {
            "workflow_id": workflow_id,
            "topic": workflow["topic"],
            "description": workflow["description"],
            "status": workflow["status"],
            "current_stage": workflow["current_stage"],
            "stages": workflow["stages"],
            "test_mode": workflow["test_mode"],
            "created_at": workflow["created_at"]
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get patent workflow status: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get patent workflow status: {str(e)}")

@app.get("/patent/{workflow_id}/results")
async def get_patent_workflow_results(workflow_id: str):
    """Get results of a completed patent workflow"""
    try:
        if not hasattr(app.state, 'workflows') or workflow_id not in app.state.workflows:
            raise HTTPException(status_code=404, detail="Patent workflow not found")
        
        workflow = app.state.workflows[workflow_id]
        if workflow["status"] != "completed":
            return {
                "workflow_id": workflow_id,
                "status": workflow["status"],
                "message": "Workflow is not yet completed",
                "current_stage": workflow["current_stage"]
            }
        
        return {
            "workflow_id": workflow_id,
            "topic": workflow["topic"],
            "description": workflow["description"],
            "status": workflow["status"],
            "results": workflow["results"],
            "completed_at": workflow.get("completed_at"),
            "patent_file_path": workflow.get("patent_file_path"),
            "download_url": workflow.get("download_url")
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get patent workflow results: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get patent workflow results: {str(e)}")

@app.post("/patent/{workflow_id}/restart")
async def restart_patent_workflow(workflow_id: str):
    """Restart a patent workflow"""
    try:
        if not hasattr(app.state, 'workflows') or workflow_id not in app.state.workflows:
            raise HTTPException(status_code=404, detail="Patent workflow not found")
        
        workflow = app.state.workflows[workflow_id]
        
        # Reset workflow state
        workflow["status"] = "restarted"
        workflow["current_stage"] = "planning"
        for stage in workflow["stages"]:
            workflow["stages"][stage] = {"status": "pending", "started_at": None, "completed_at": None}
        workflow["results"] = {}
        
        # Start workflow execution in background
        background_tasks = BackgroundTasks()
        background_tasks.add_task(execute_patent_workflow, workflow_id, workflow["topic"], workflow["description"], workflow["test_mode"])
        
        return {
            "workflow_id": workflow_id,
            "status": "restarted",
            "message": "Patent workflow restarted successfully"
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to restart patent workflow: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to restart patent workflow: {str(e)}")

@app.delete("/patent/{workflow_id}")
async def delete_patent_workflow(workflow_id: str):
    """Delete a patent workflow"""
    try:
        if not hasattr(app.state, 'workflows') or workflow_id not in app.state.workflows:
            raise HTTPException(status_code=404, detail="Patent workflow not found")
        
        del app.state.workflows[workflow_id]
        return {
            "workflow_id": workflow_id,
            "status": "deleted",
            "message": "Patent workflow deleted successfully"
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to delete patent workflow: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to delete patent workflow: {str(e)}")

@app.websocket("/ws/workflow/{workflow_id}")
async def websocket_workflow_updates(websocket: WebSocket, workflow_id: str):
    """WebSocket endpoint for real-time workflow updates"""
    try:
        await manager.connect(websocket, workflow_id)
        
        # Send initial connection confirmation
        await manager.send_personal_message(
            json.dumps({
                "type": "connection_established",
                "workflow_id": workflow_id,
                "message": "Connected to workflow updates",
                "timestamp": time.time()
            }),
            websocket
        )
        
        # Keep connection alive and handle messages
        while True:
            try:
                # Wait for any message from client (ping/pong)
                data = await websocket.receive_text()
                if data == "ping":
                    await manager.send_personal_message(
                        json.dumps({
                            "type": "pong",
                            "timestamp": time.time()
                        }),
                        websocket
                    )
            except WebSocketDisconnect:
                manager.disconnect(websocket, workflow_id)
                break
            except Exception as e:
                logger.error(f"WebSocket error: {e}")
                break
                
    except Exception as e:
        logger.error(f"WebSocket connection failed: {e}")
        manager.disconnect(websocket, workflow_id)

@app.get("/workflow/{workflow_id}/progress")
async def get_workflow_progress(workflow_id: str):
    """Get real-time workflow progress"""
    try:
        if not hasattr(app.state, 'workflows') or workflow_id not in app.state.workflows:
            raise HTTPException(status_code=404, detail="Workflow not found")
        
        workflow = app.state.workflows[workflow_id]
        
        # Calculate progress
        total_stages = len(workflow["stages"])
        completed_stages = len([s for s in workflow["stages"].values() if s["status"] == "completed"])
        current_stage = workflow.get("current_stage", "unknown")
        
        progress = {
            "workflow_id": workflow_id,
            "topic": workflow.get("topic"),
            "status": workflow.get("status"),
            "current_stage": current_stage,
            "progress": f"{completed_stages}/{total_stages}",
            "percentage": round((completed_stages / total_stages) * 100, 1),
            "completed_stages": completed_stages,
            "total_stages": total_stages,
            "started_at": workflow.get("created_at"),
            "estimated_completion": None
        }
        
        # Add estimated completion time for running workflows
        if workflow["status"] == "running":
            if completed_stages > 0:
                # Estimate based on average time per stage
                avg_time_per_stage = 2.0  # seconds in test mode
                remaining_stages = total_stages - completed_stages
                estimated_remaining = remaining_stages * avg_time_per_stage
                progress["estimated_completion"] = f"~{estimated_remaining:.1f} seconds"
        
        return progress
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get workflow progress: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get workflow progress: {str(e)}")

@app.get("/workflow/{workflow_id}/stages")
async def get_workflow_stages(workflow_id: str):
    """Get all stage files for a workflow"""
    try:
        if not hasattr(app.state, 'workflows') or workflow_id not in app.state.workflows:
            raise HTTPException(status_code=404, detail="Workflow not found")
        
        workflow = app.state.workflows[workflow_id]
        workflow_dir = workflow.get("workflow_directory")
        
        if not workflow_dir or not os.path.exists(workflow_dir):
            raise HTTPException(status_code=404, detail="Workflow directory not found")
        
        # Read stage index
        index_file = f"{workflow_dir}/stage_index.json"
        if os.path.exists(index_file):
            with open(index_file, 'r', encoding='utf-8') as f:
                stage_index = json.load(f)
        else:
            stage_index = {"stages": {}}
        
        # Read metadata
        metadata_file = f"{workflow_dir}/metadata.json"
        metadata = {}
        if os.path.exists(metadata_file):
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
        
        # List all files in directory
        files = []
        if os.path.exists(workflow_dir):
            for filename in os.listdir(workflow_dir):
                if filename.endswith('.md'):
                    file_path = f"{workflow_dir}/{filename}"
                    file_stat = os.stat(file_path)
                    files.append({
                        "filename": filename,
                        "size": file_stat.st_size,
                        "modified": time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(file_stat.st_mtime))
                    })
        
        return {
            "workflow_id": workflow_id,
            "topic": workflow.get("topic"),
            "workflow_directory": workflow_dir,
            "metadata": metadata,
            "stage_index": stage_index,
            "files": files
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get workflow stages: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get workflow stages: {str(e)}")

@app.get("/download/workflow/{workflow_id}")
async def download_workflow_directory(workflow_id: str):
    """Download entire workflow directory as a zip file"""
    try:
        if not hasattr(app.state, 'workflows') or workflow_id not in app.state.workflows:
            raise HTTPException(status_code=404, detail="Workflow not found")
        
        workflow = app.state.workflows[workflow_id]
        workflow_dir = workflow.get("workflow_directory")
        
        if not workflow_dir or not os.path.exists(workflow_dir):
            raise HTTPException(status_code=404, detail="Workflow directory not found")
        
        # Create zip file of the entire workflow directory
        import zipfile
        import tempfile
        
        # Create temporary zip file
        with tempfile.NamedTemporaryFile(delete=False, suffix='.zip') as tmp_zip:
            with zipfile.ZipFile(tmp_zip.name, 'w', zipfile.ZIP_DEFLATED) as zipf:
                # Walk through the workflow directory
                for root, dirs, files in os.walk(workflow_dir):
                    for file in files:
                        file_path = os.path.join(root, file)
                        # Calculate relative path for zip
                        arcname = os.path.relpath(file_path, workflow_dir)
                        zipf.write(file_path, arcname)
            
            # Return zip file for download
            return FileResponse(
                path=tmp_zip.name,
                filename=f"workflow_{workflow_id}.zip",
                media_type="application/zip",
                headers={"Content-Disposition": f"attachment; filename=workflow_{workflow_id}.zip"}
            )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to download workflow directory: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to download workflow directory: {str(e)}")

@app.get("/download/patent/{workflow_id}")
async def download_patent_file(workflow_id: str):
    """Download final patent file for a completed workflow"""
    try:
        if not hasattr(app.state, 'workflows') or workflow_id not in app.state.workflows:
            raise HTTPException(status_code=404, detail="Patent workflow not found")
        
        workflow = app.state.workflows[workflow_id]
        if workflow["status"] != "completed":
            raise HTTPException(status_code=400, detail="Workflow is not yet completed")
        
        patent_file_path = workflow.get("patent_file_path")
        if not patent_file_path:
            raise HTTPException(status_code=404, detail="Patent file not found")
        
        # Check if file exists
        if not os.path.exists(patent_file_path):
            raise HTTPException(status_code=404, detail="Patent file not found on disk")
        
        # Return file for download
        try:
            return FileResponse(
                path=patent_file_path,
                filename=f"final_patent_{workflow_id}.md",
                media_type="text/markdown"
            )
        except Exception as file_error:
            logger.error(f"FileResponse error: {file_error}")
            # Fallback: return file content as text
            with open(patent_file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            return JSONResponse(
                content={"file_content": content, "filename": f"final_patent_{workflow_id}.md"},
                headers={"Content-Disposition": f"attachment; filename=final_patent_{workflow_id}.md"}
            )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to download patent file: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to download patent file: {str(e)}")

@app.get("/patents")
async def list_patent_workflows():
    """List all patent workflows"""
    try:
        if not hasattr(app.state, 'workflows'):
            return {"patent_workflows": [], "total": 0, "summary": {}}
        
        workflows = list(app.state.workflows.values())
        # Filter for patent workflows (workflow_type == "patent")
        patent_workflows = [w for w in workflows if w.get("workflow_type") == "patent"]
        
        # Add summary information for patents
        summary = {
            "total_patents": len(patent_workflows),
            "by_topic": {},
            "by_status": {},
            "by_test_mode": {"test": 0, "real": 0}
        }
        
        for workflow in patent_workflows:
            topic = workflow.get("topic", "Unknown")
            status = workflow.get("status", "Unknown")
            test_mode = workflow.get("test_mode", False)
            
            # Count by topic
            if topic not in summary["by_topic"]:
                summary["by_topic"][topic] = 0
            summary["by_topic"][topic] += 1
            
            # Count by status
            if status not in summary["by_status"]:
                summary["by_status"][status] = 0
            summary["by_status"][status] += 1
            
            # Count by test mode
            if test_mode:
                summary["by_test_mode"]["test"] += 1
            else:
                summary["by_test_mode"]["real"] += 1
        
        return {
            "patent_workflows": patent_workflows,
            "total": len(patent_workflows),
            "summary": summary
        }
    except Exception as e:
        logger.error(f"Failed to list patent workflows: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to list patent workflows: {str(e)}")

# ============================================================================
# COORDINATOR ENDPOINTS
# ============================================================================

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "Unified Patent Agent System v2.0.0", 
        "status": "running",
        "test_mode": False,  # Root endpoint always shows real mode
        "services": {
            "coordinator": "/coordinator/*",
            "agents": {
                "planner": "/agents/planner/*",
                "searcher": "/agents/searcher/*",
                "discussion": "/agents/discussion/*",
                "writer": "/agents/writer/*",
                "reviewer": "/agents/reviewer/*",
                "rewriter": "/agents/rewriter/*"
            }
        }
    }

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "version": "2.0.0",
        "test_mode": False,  # Health check always shows real mode
        "active_workflows": len(workflow_manager.workflows),
        "services": ["coordinator", "planner", "searcher", "discussion", "writer", "reviewer", "rewriter"],
        "timestamp": time.time()
    }

@app.get("/test-mode")
async def get_test_mode():
    """Get test mode configuration - DEPRECATED: Use workflow-specific test_mode instead"""
    return {
        "test_mode": TEST_MODE,
        "description": "DEPRECATED: Global test mode configuration. Use workflow-specific test_mode parameter instead.",
        "warning": "This global configuration is deprecated. Test mode is now controlled per workflow."
    }

@app.post("/test-mode")
async def set_test_mode(test_config: Dict[str, Any]):
    """Set test mode configuration - DEPRECATED: Use workflow-specific test_mode instead"""
    global TEST_MODE
    TEST_MODE.update(test_config)
    logger.warning(f"âš ï¸ DEPRECATED: Global test mode updated: {TEST_MODE}. Use workflow-specific test_mode instead.")
    return {
        "message": "DEPRECATED: Global test mode configuration updated. Use workflow-specific test_mode instead.",
        "test_mode": TEST_MODE,
        "warning": "This global configuration is deprecated. Test mode is now controlled per workflow."
    }

# Coordinator endpoints
@app.post("/coordinator/workflow/start", response_model=WorkflowResponse)
async def start_workflow(request: WorkflowRequest, background_tasks: BackgroundTasks):
    """Start a new patent workflow"""
    try:
        logger.info(f"ğŸš€ Starting patent workflow in {'TEST' if request.test_mode else 'REAL'} mode")
        logger.info(f"ğŸ“ Topic: {request.topic}")
        logger.info(f"ğŸ”§ Test mode enabled: {request.test_mode}")
        
        # Only support patent workflows
        if request.workflow_type != "patent":
            raise HTTPException(
                status_code=400, 
                detail=f"Only patent workflows are supported. Received workflow_type: {request.workflow_type}"
            )
        
        # Create patent workflow
        workflow_id = str(uuid.uuid4())
        
        # Initialize patent workflow state
        workflow_state = {
            "workflow_id": workflow_id,
            "topic": request.topic,
            "description": request.description or f"Patent for topic: {request.topic}",
            "workflow_type": "patent",
            "test_mode": request.test_mode,
            "status": "created",
            "created_at": time.time(),
            "stages": {
                "planning": {"status": "pending", "started_at": None, "completed_at": None},
                "search": {"status": "pending", "started_at": None, "completed_at": None},
                "discussion": {"status": "pending", "started_at": None, "completed_at": None},
                "drafting": {"status": "pending", "started_at": None, "completed_at": None},
                "review": {"status": "pending", "started_at": None, "completed_at": None},
                "rewrite": {"status": "pending", "started_at": None, "completed_at": None}
            },
            "results": {},
            "current_stage": "planning"
        }
        
        # Store workflow in memory
        if not hasattr(app.state, 'workflows'):
            app.state.workflows = {}
        app.state.workflows[workflow_id] = workflow_state
        
        # Start patent workflow execution in background
        background_tasks.add_task(execute_patent_workflow, workflow_id, request.topic, request.description, request.test_mode)
        
        return WorkflowResponse(
            workflow_id=workflow_id,
            status="started",
            message=f"Patent workflow started successfully for topic: {request.topic} (test_mode: {request.test_mode})"
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to start patent workflow: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to start patent workflow: {str(e)}")

@app.get("/coordinator/workflow/{workflow_id}/status", response_model=WorkflowStatus)
async def get_workflow_status(workflow_id: str):
    """Get patent workflow status and progress"""
    try:
        # Only support patent workflows
        if not hasattr(app.state, 'workflows') or workflow_id not in app.state.workflows:
            raise HTTPException(status_code=404, detail="Patent workflow not found")
        
        workflow = app.state.workflows[workflow_id]
        if workflow.get("workflow_type") != "patent":
            raise HTTPException(status_code=400, detail="Only patent workflows are supported")
        
        return {
            "workflow_id": workflow_id,
            "topic": workflow["topic"],
            "description": workflow["description"],
            "status": workflow["status"],
            "current_stage": workflow["current_stage"],
            "stages": workflow["stages"],
            "test_mode": workflow["test_mode"],
            "created_at": workflow["created_at"]
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get patent workflow status: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get patent workflow status: {str(e)}")

@app.get("/coordinator/workflow/{workflow_id}/results")
async def get_workflow_results(workflow_id: str):
    """Get patent workflow results"""
    try:
        # Check if this is a patent workflow
        if hasattr(app.state, 'workflows') and workflow_id in app.state.workflows:
            workflow = app.state.workflows[workflow_id]
            if workflow.get("workflow_type") == "patent":
                if workflow["status"] != "completed":
                    return {
                        "workflow_id": workflow_id,
                        "status": workflow["status"],
                        "message": "Workflow is not yet completed",
                        "current_stage": workflow["current_stage"]
                    }
                
                return {
                    "workflow_id": workflow_id,
                    "topic": workflow["topic"],
                    "description": workflow["description"],
                    "status": workflow["status"],
                    "results": workflow["results"],
                    "completed_at": workflow.get("completed_at"),
                    "test_mode": workflow["test_mode"],
                    "patent_file_path": workflow.get("patent_file_path"),
                    "download_url": workflow.get("download_url")
                }
        
        # Use regular workflow manager
        results = workflow_manager.get_workflow_results(workflow_id)
        return {"workflow_id": workflow_id, "results": results, "test_mode": workflow.get("test_mode", False)}
    except KeyError:
        raise HTTPException(status_code=404, detail="Workflow not found")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get results: {str(e)}")

@app.post("/coordinator/workflow/{workflow_id}/restart")
async def restart_workflow(workflow_id: str, background_tasks: BackgroundTasks):
    """Restart a patent workflow"""
    try:
        # Only support patent workflows
        if not hasattr(app.state, 'workflows') or workflow_id not in app.state.workflows:
            raise HTTPException(status_code=404, detail="Patent workflow not found")
        
        workflow = app.state.workflows[workflow_id]
        if workflow.get("workflow_type") != "patent":
            raise HTTPException(status_code=400, detail="Only patent workflows are supported")
        
        # Reset workflow state
        workflow["status"] = "restarted"
        workflow["current_stage"] = "planning"
        for stage in workflow["stages"]:
            workflow["stages"][stage] = {"status": "pending", "started_at": None, "completed_at": None}
        workflow["results"] = {}
        
        # Start workflow execution in background
        background_tasks.add_task(execute_patent_workflow, workflow_id, workflow["topic"], workflow["description"], workflow["test_mode"])
        
        return {"workflow_id": workflow_id, "status": "restarted", "message": "Patent workflow restarted"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to restart patent workflow: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to restart patent workflow: {str(e)}")

@app.get("/coordinator/workflows")
async def list_workflows():
    """List all patent workflows"""
    try:
        # Only support patent workflows
        patent_workflows = []
        if hasattr(app.state, 'workflows'):
            for workflow in app.state.workflows.values():
                if workflow.get("workflow_type") == "patent":
                    patent_workflows.append({
                        "workflow_id": workflow["workflow_id"],
                        "topic": workflow["topic"],
                        "description": workflow["description"],
                        "workflow_type": "patent",
                        "status": workflow["status"],
                        "current_stage": workflow["current_stage"],
                        "test_mode": workflow["test_mode"],
                        "created_at": workflow["created_at"]
                    })
        
        return {
            "workflows": patent_workflows, 
            "patent_workflows": patent_workflows,
            "total_workflows": len(patent_workflows),
            "test_mode": False  # List endpoint always shows real mode
        }
    except Exception as e:
        logger.error(f"Failed to list patent workflows: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to list patent workflows: {str(e)}")

@app.delete("/coordinator/workflow/{workflow_id}")
async def delete_workflow(workflow_id: str):
    """Delete a patent workflow"""
    try:
        # Only support patent workflows
        if not hasattr(app.state, 'workflows') or workflow_id not in app.state.workflows:
            raise HTTPException(status_code=404, detail="Patent workflow not found")
        
        workflow = app.state.workflows[workflow_id]
        if workflow.get("workflow_type") != "patent":
            raise HTTPException(status_code=400, detail="Only patent workflows are supported")
        
        del app.state.workflows[workflow_id]
        return {"workflow_id": workflow_id, "status": "deleted", "message": "Patent workflow deleted"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to delete patent workflow: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to delete patent workflow: {str(e)}")

# ============================================================================
# AGENT ENDPOINTS
# ============================================================================

class TaskRequest(BaseModel):
    """Task request model"""
    task_id: str
    workflow_id: str
    stage_name: str
    topic: str
    description: str
    test_mode: bool = False
    previous_results: Dict[str, Any] = {}
    context: Dict[str, Any] = {}

class TaskResponse(BaseModel):
    """Task response model"""
    task_id: str
    status: str
    result: Dict[str, Any]
    message: str
    test_mode: bool

# Planner Agent
@app.get("/agents/planner/health")
async def planner_health():
    """Planner agent health check"""
    return {
        "status": "healthy",
        "service": "planner_agent",
        "test_mode": False,  # Health check always shows real mode
        "capabilities": ["patent_planning", "strategy_development", "risk_assessment", "timeline_planning"],
        "timestamp": time.time()
    }

@app.post("/agents/planner/execute", response_model=TaskResponse)
async def planner_execute(request: TaskRequest):
    """Execute planner agent task"""
    try:
        logger.info(f"ğŸ“‹ Planner Agent received task: {request.task_id}")
        logger.info(f"ğŸ”§ Test mode: {request.test_mode}")
        
        # DEBUG: è¯¦ç»†æ£€æŸ¥æ¥æ”¶åˆ°çš„å‚æ•°
        logger.info(f"ğŸ” DEBUG API: æ¥æ”¶åˆ°çš„request.test_mode = {request.test_mode}")
        logger.info(f"ğŸ” DEBUG API: type(request.test_mode) = {type(request.test_mode)}")
        logger.info(f"ğŸ” DEBUG API: request.test_mode == False = {request.test_mode == False}")
        logger.info(f"ğŸ” DEBUG API: request.test_mode == True = {request.test_mode == True}")
        
        result = await execute_planner_task(request)
        logger.info(f"ğŸ” DEBUG: execute_planner_task returned result with test_mode: {result.get('test_mode', 'NOT_FOUND')}")
        logger.info(f"ğŸ” DEBUG: request.test_mode: {request.test_mode}")
        logger.info(f"ğŸ” DEBUG: result type: {type(result)}")
        logger.info(f"ğŸ” DEBUG: result keys: {list(result.keys()) if isinstance(result, dict) else 'NOT_DICT'}")
        
        return TaskResponse(
            task_id=request.task_id,
            status="completed",
            result=result,
            message=f"Patent planning completed successfully in {'TEST' if request.test_mode else 'REAL'} mode",
            test_mode=request.test_mode
        )
    except Exception as e:
        logger.error(f"âŒ Planner Agent failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Task execution failed: {str(e)}")

# Searcher Agent
@app.get("/agents/searcher/health")
async def searcher_health():
    """Searcher agent health check"""
    return {
        "status": "healthy",
        "service": "searcher_agent",
        "test_mode": False,  # Health check always shows real mode
        "capabilities": ["prior_art_search", "patent_analysis", "competitive_research", "novelty_assessment"],
        "timestamp": time.time()
    }

@app.post("/agents/searcher/execute", response_model=TaskResponse)
async def searcher_execute(request: TaskRequest):
    """Execute searcher agent task"""
    try:
        logger.info(f"ğŸ” Searcher Agent received task: {request.task_id}")
        logger.info(f"ğŸ”§ Test mode: {request.test_mode}")
        
        result = await execute_searcher_task(request)
        return TaskResponse(
            task_id=request.task_id,
            status="completed",
            result=result,
            message=f"Prior art search completed successfully in {'TEST' if request.test_mode else 'REAL'} mode",
            test_mode=request.test_mode
        )
    except Exception as e:
        logger.error(f"âŒ Searcher Agent failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Task execution failed: {str(e)}")

# Discussion Agent
@app.get("/agents/discussion/health")
async def discussion_health():
    """Discussion agent health check"""
    return {
        "status": "healthy",
        "service": "discussion_agent",
        "test_mode": False,  # Health check always shows real mode
        "capabilities": ["innovation_discussion", "idea_generation", "technical_analysis"],
        "timestamp": time.time()
    }

@app.post("/agents/discussion/execute", response_model=TaskResponse)
async def discussion_execute(request: TaskRequest):
    """Execute discussion agent task"""
    try:
        logger.info(f"ğŸ’¬ Discussion Agent received task: {request.task_id}")
        logger.info(f"ğŸ”§ Test mode: {request.test_mode}")
        
        result = await execute_discussion_task(request)
        return TaskResponse(
            task_id=request.task_id,
            status="completed",
            result=result,
            message=f"Innovation discussion completed successfully in {'TEST' if request.test_mode else 'REAL'} mode",
            test_mode=request.test_mode
        )
    except Exception as e:
        logger.error(f"âŒ Discussion Agent failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Task execution failed: {str(e)}")

# Writer Agent
@app.get("/agents/writer/health")
async def writer_health():
    """Writer agent health check"""
    return {
        "status": "healthy",
        "service": "writer_agent",
        "test_mode": False,  # Health check always shows real mode
        "capabilities": ["patent_drafting", "technical_writing", "claim_writing", "legal_compliance"],
        "timestamp": time.time()
    }

@app.post("/agents/writer/execute", response_model=TaskResponse)
async def writer_execute(request: TaskRequest):
    """Execute writer agent task"""
    try:
        logger.info(f"âœï¸ Writer Agent received task: {request.task_id}")
        logger.info(f"ğŸ”§ Test mode: {request.test_mode}")
        
        result = await execute_writer_task(request)
        return TaskResponse(
            task_id=request.task_id,
            status="completed",
            result=result,
            message=f"Patent drafting completed successfully in {'TEST' if request.test_mode else 'REAL'} mode",
            test_mode=request.test_mode
        )
    except Exception as e:
        logger.error(f"âŒ Writer Agent failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Task execution failed: {str(e)}")

# Reviewer Agent
@app.get("/agents/reviewer/health")
async def reviewer_health():
    """Reviewer agent health check"""
    return {
        "status": "healthy",
        "service": "reviewer_agent",
        "test_mode": False,  # Health check always shows real mode
        "capabilities": ["quality_review", "compliance_check", "feedback_generation"],
        "timestamp": time.time()
    }

@app.post("/agents/reviewer/execute", response_model=TaskResponse)
async def reviewer_execute(request: TaskRequest):
    """Execute reviewer agent task"""
    try:
        logger.info(f"ğŸ” Reviewer Agent received task: {request.task_id}")
        logger.info(f"ğŸ”§ Test mode: {request.test_mode}")
        
        result = await execute_reviewer_task(request)
        return TaskResponse(
            task_id=request.task_id,
            status="completed",
            result=result,
            message=f"Quality review completed successfully in {'TEST' if request.test_mode else 'REAL'} mode",
            test_mode=request.test_mode
        )
    except Exception as e:
        logger.error(f"âŒ Reviewer Agent failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Task execution failed: {str(e)}")

# Rewriter Agent
@app.get("/agents/rewriter/health")
async def rewriter_health():
    """Rewriter agent health check"""
    return {
        "status": "healthy",
        "service": "rewriter_agent",
        "test_mode": False,  # Health check always shows real mode
        "capabilities": ["patent_rewriting", "improvement_generation", "final_polish"],
        "timestamp": time.time()
    }

@app.post("/agents/rewriter/execute", response_model=TaskResponse)
async def rewriter_execute(request: TaskRequest):
    """Execute rewriter agent task"""
    try:
        logger.info(f"âœï¸ Rewriter Agent received task: {request.task_id}")
        logger.info(f"ğŸ”§ Test mode: {request.test_mode}")
        
        result = await execute_rewriter_task(request)
        return TaskResponse(
            task_id=request.task_id,
            status="completed",
            result=result,
            message=f"Patent rewriting completed successfully in {'TEST' if request.test_mode else 'REAL'} mode",
            test_mode=request.test_mode
        )
    except Exception as e:
        logger.error(f"âŒ Rewriter Agent failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Task execution failed: {str(e)}")

# ============================================================================
# COMPRESSION AGENT ENDPOINTS
# ============================================================================

# Compression Agent
@app.get("/agents/compressor/health")
async def compressor_health():
    """Compression agent health check"""
    return {
        "status": "healthy",
        "service": "compression_agent",
        "test_mode": False,  # Health check always shows real mode
        "capabilities": ["context_compression", "content_summarization", "key_insight_extraction", "unified_content_preservation"],
        "timestamp": time.time()
    }

@app.post("/agents/compressor/execute", response_model=TaskResponse)
async def compressor_execute(request: TaskRequest):
    """Execute compression agent task"""
    try:
        logger.info(f"ğŸ—œï¸ Compression Agent received task: {request.task_id}")
        logger.info(f"ğŸ”§ Test mode: {request.test_mode}")
        
        result = await execute_compression_task(request)
        return TaskResponse(
            task_id=request.task_id,
            status="completed",
            result=result,
            message=f"Context compression completed successfully in {'TEST' if request.test_mode else 'REAL'} mode",
            test_mode=request.test_mode
        )
    except Exception as e:
        logger.error(f"âŒ Compression Agent failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Task execution failed: {str(e)}")

# ============================================================================
# AGENT TASK EXECUTION FUNCTIONS
# ============================================================================

async def execute_planner_task(request: TaskRequest) -> Dict[str, Any]:
    """Execute planner task using old system prompts with workflow isolation"""
    topic = request.topic
    description = request.description
    workflow_id = request.workflow_id
    context = request.context
    
    logger.info(f"ğŸš€ Starting patent planning for workflow {workflow_id}: {topic}")
    logger.info(f"ğŸ”§ Test mode: {request.test_mode}")
    logger.info(f"ğŸ”’ Workflow isolation: {context.get('isolation_level', 'unknown')}")
    
    # DEBUG: è¯¦ç»†è¿½è¸ªtest_mode
    logger.info(f"ğŸ” DEBUG STEP 1: request.test_mode = {request.test_mode}")
    logger.info(f"ğŸ” DEBUG STEP 1: type(request.test_mode) = {type(request.test_mode)}")
    logger.info(f"ğŸ” DEBUG STEP 1: request.test_mode == False = {request.test_mode == False}")
    logger.info(f"ğŸ” DEBUG STEP 1: request.test_mode == True = {request.test_mode == True}")
    
    # Validate workflow context
    if context.get("workflow_id") != workflow_id:
        logger.warning(f"âš ï¸ Workflow ID mismatch in context: expected {workflow_id}, got {context.get('workflow_id')}")
    
    # Add test mode delay
    if request.test_mode:
        await asyncio.sleep(0.5)  # Simulate processing time
        logger.info(f"â±ï¸ Test mode delay: 0.5s")
    
    # Mock execution with old system prompts
    analysis = await analyze_patent_topic(topic, description)
    strategy = await develop_strategy(topic, description, analysis)
    phases = await create_development_phases(strategy)
    risk_assessment = await assess_competitive_risks(strategy, analysis)
    timeline_estimate = await estimate_timeline(phases)
    resource_requirements = await estimate_resources(phases)
    success_probability = await calculate_success_probability(strategy, risk_assessment)
    
    final_strategy = {
        "topic": topic,
        "description": description,
        "novelty_score": analysis.get("novelty_score", 8.5),
        "inventive_step_score": analysis.get("inventive_step_score", 7.8),
        "patentability_assessment": analysis.get("patentability_assessment", "Strong"),
        "development_phases": phases,
        "key_innovation_areas": strategy.get("key_innovation_areas", []),
        "competitive_analysis": risk_assessment.get("competitive_analysis", {}),
        "risk_assessment": risk_assessment,
        "timeline_estimate": timeline_estimate,
        "resource_requirements": resource_requirements,
        "success_probability": success_probability
    }
    
    # DEBUG: åœ¨è¿”å›ç»“æœæ„å»ºä¹‹å‰æ£€æŸ¥test_mode
    logger.info(f"ğŸ” DEBUG STEP 2: å‡†å¤‡æ„å»ºè¿”å›ç»“æœ")
    logger.info(f"ğŸ” DEBUG STEP 2: request.test_mode = {request.test_mode}")
    logger.info(f"ğŸ” DEBUG STEP 2: execution_time è®¡ç®—: {0.5 if request.test_mode else 1.0}")
    logger.info(f"ğŸ” DEBUG STEP 2: mock_delay_applied è®¡ç®—: {0.5 if request.test_mode else 0}")
    
    result_dict = {
        "workflow_id": workflow_id,  # Include workflow ID in result
        "strategy": final_strategy,
        "analysis": analysis,
        "recommendations": analysis.get("recommendations", []),
        "execution_time": 0.5 if request.test_mode else 1.0,
        "test_mode": request.test_mode,
        "mock_delay_applied": 0.5 if request.test_mode else 0,
        "isolation_timestamp": time.time()
    }
    
    # DEBUG: æ£€æŸ¥æ„å»ºåçš„ç»“æœ
    logger.info(f"ğŸ” DEBUG STEP 3: æ„å»ºå®Œæˆçš„ç»“æœ")
    logger.info(f"ğŸ” DEBUG STEP 3: result_dict['test_mode'] = {result_dict['test_mode']}")
    logger.info(f"ğŸ” DEBUG STEP 3: result_dict keys = {list(result_dict.keys())}")
    
    return result_dict

async def execute_searcher_task(request: TaskRequest) -> Dict[str, Any]:
    """Execute searcher task using old system prompts with workflow isolation"""
    topic = request.topic
    description = request.description
    workflow_id = request.workflow_id
    context = request.context
    
    logger.info(f"ğŸš€ Starting prior art search for workflow {workflow_id}: {topic}")
    logger.info(f"ğŸ”§ Test mode: {request.test_mode}")
    logger.info(f"ğŸ”’ Workflow isolation: {context.get('isolation_level', 'unknown')}")
    
    # Validate workflow context
    if context.get("workflow_id") != workflow_id:
        logger.warning(f"âš ï¸ Workflow ID mismatch in context: expected {workflow_id}, got {context.get('workflow_id')}")
    
    # Add test mode delay
    if request.test_mode:
        await asyncio.sleep(0.5)  # Simulate processing time
        logger.info(f"â±ï¸ Test mode delay: 0.5s")
    
    keywords = await extract_keywords(topic, description)
    search_results = await conduct_prior_art_search(topic, keywords, {})
    analysis = await analyze_search_results(search_results, topic)
    novelty_assessment = await assess_novelty(search_results, analysis)
    recommendations = await generate_recommendations(search_results, analysis, novelty_assessment)
    
    # ç¡®ä¿search_resultsæ ¼å¼ä¸åç»­æ™ºèƒ½ä½“å…¼å®¹
    compatible_search_results = _ensure_search_results_compatibility(search_results)
    
    search_report = {
        "query": {"topic": topic, "keywords": keywords, "date_range": "Last 20 years", "jurisdiction": "Global", "max_results": 50},
        "results": compatible_search_results,  # ä½¿ç”¨å…¼å®¹æ ¼å¼
        "analysis": analysis,
        "recommendations": recommendations,
        "risk_assessment": novelty_assessment.get("risk_assessment", {}),
        "novelty_score": novelty_assessment.get("novelty_score", 8.0)
    }
    
    return {
        "workflow_id": workflow_id,  # Include workflow ID in result
        "search_results": search_report,
        "patents_found": len(compatible_search_results),
        "novelty_score": novelty_assessment.get("novelty_score", 8.0),
        "risk_level": novelty_assessment.get("risk_level", "Low"),
        "recommendations": recommendations,
        "execution_time": 0.5 if request.test_mode else 1.0,
        "test_mode": request.test_mode,
        "mock_delay_applied": 0.5 if request.test_mode else 0,
        "isolation_timestamp": time.time()
    }

def _ensure_search_results_compatibility(search_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """ç¡®ä¿æ£€ç´¢ç»“æœä¸åç»­æ™ºèƒ½ä½“å®Œå…¨å…¼å®¹"""
    compatible_results = []
    
    for result in search_results:
        # ä¿ç•™æ ¸å¿ƒå¿…éœ€å­—æ®µ
        compatible_result = {
            "patent_id": result.get("patent_id", "UNKNOWN"),
            "title": result.get("title", "æ— æ ‡é¢˜"),
            "abstract": result.get("abstract", "æ— æ‘˜è¦"),
            "filing_date": result.get("filing_date", "N/A"),
            "publication_date": result.get("publication_date", "N/A"),
            "assignee": result.get("assignee", "Various"),
            "relevance_score": result.get("relevance_score", 0.7),
            "similarity_analysis": result.get("similarity_analysis", {
                "concept_overlap": "å¾…åˆ†æ",
                "technical_similarity": "å¾…åˆ†æ", 
                "implementation_differences": "å¾…åˆ†æ"
            })
        }
        
        # æ·»åŠ GLMåˆ†æç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if "glm_final_analysis" in result:
            compatible_result["glm_analysis"] = result["glm_final_analysis"]
            compatible_result["enhanced_by_glm"] = True
        elif "glm_analysis" in result:
            compatible_result["glm_analysis"] = result["glm_analysis"]
            compatible_result["enhanced_by_glm"] = True
        
        # æ·»åŠ è¿­ä»£è½®æ¬¡ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if "analysis_round" in result:
            compatible_result["analysis_round"] = result["analysis_round"]
        
        # æ·»åŠ æŠ€æœ¯æ´å¯Ÿï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if "technical_insights" in result:
            compatible_result["technical_insights"] = result["technical_insights"]
        
        compatible_results.append(compatible_result)
    
    logger.info(f"âœ… æ£€ç´¢ç»“æœå…¼å®¹æ€§æ£€æŸ¥å®Œæˆï¼Œ{len(compatible_results)} ä¸ªç»“æœå·²æ ‡å‡†åŒ–")
    return compatible_results

async def execute_discussion_task(request: TaskRequest) -> Dict[str, Any]:
    """Execute discussion task using GLM API or fallback to mock"""
    topic = request.topic
    previous_results = request.previous_results
    
    logger.info(f"ğŸš€ Starting innovation discussion for: {topic}")
    logger.info(f"ğŸ”§ Test mode: {request.test_mode}")
    logger.info(f"ğŸ“‹ Previous results keys: {list(previous_results.keys())}")
    
    # Add test mode delay
    if request.test_mode:
        await asyncio.sleep(0.5)  # Simulate processing time
        logger.info(f"â±ï¸ Test mode delay: 0.5s")
    
    # Extract core strategy from planning stage
    planning_result = previous_results.get("planning", {})
    search_result = previous_results.get("search", {})
    
    # ä¿®å¤ï¼šæ­£ç¡®è§£ææ•°æ®ç»“æ„å¹¶æ·»åŠ è°ƒè¯•ä¿¡æ¯
    logger.info(f"ğŸ” Planning result type: {type(planning_result)}")
    logger.info(f"ğŸ” Planning result keys: {list(planning_result.keys()) if isinstance(planning_result, dict) else 'Not a dict'}")
    logger.info(f"ğŸ” Search result type: {type(search_result)}")
    logger.info(f"ğŸ” Search result keys: {list(search_result.keys()) if isinstance(search_result, dict) else 'Not a dict'}")
    
    # ä¿®å¤ï¼šæ­£ç¡®è§£ææ•°æ®ç»“æ„
    planning_strategy = planning_result.get("strategy", {}) if isinstance(planning_result, dict) else {}
    search_results = search_result.get("search_results", {}) if isinstance(search_result, dict) else {}
    
    # å¦‚æœsearch_resultsä¸å­˜åœ¨ï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„å­—æ®µå
    if not search_results and isinstance(search_result, dict):
        search_results = search_result.get("results", {})  # å°è¯•resultså­—æ®µ
        if not search_results:
            search_results = search_result.get("search_data", {})  # å°è¯•search_dataå­—æ®µ
    
    # Build on previous stages' insights
    core_innovation_areas = planning_strategy.get("key_innovation_areas", [])
    novelty_score = planning_result.get("novelty_score", 8.5)
    
    # ä¿®å¤ï¼šæ­£ç¡®è·å–search_findingsï¼Œè€ƒè™‘Searché˜¶æ®µçš„å®é™…æ•°æ®ç»“æ„
    search_findings = []
    if isinstance(search_results, dict):
        search_findings = search_results.get("results", [])
    elif isinstance(search_result, dict):
        # ç›´æ¥ä»search_resultè·å–ï¼Œå› ä¸ºSearché˜¶æ®µè¿”å›çš„æ˜¯{"search_results": {...}, "results": [...]}
        search_findings = search_result.get("results", [])
        if not search_findings:
            # å°è¯•ä»search_resultså­—æ®µè·å–
            search_results_data = search_result.get("search_results", {})
            if isinstance(search_results_data, dict):
                search_findings = search_results_data.get("results", [])
    
    logger.info(f"ğŸ” æœ€ç»ˆè·å–åˆ°çš„search_findingsæ•°é‡: {len(search_findings)}")
    
    logger.info(f"ğŸ“‹ Building on planning strategy: {core_innovation_areas}")
    logger.info(f"ğŸ” Incorporating search findings: {len(search_findings)} patents found")
    logger.info(f"ğŸ“Š Planning strategy keys: {list(planning_strategy.keys())}")
    logger.info(f"ğŸ“Š Search results keys: {list(search_results.keys()) if isinstance(search_results, dict) else 'Not a dict'}")
    
    if GLM_AVAILABLE:
        try:
            logger.info("ğŸš€ ä½¿ç”¨GLM APIè¿›è¡Œåˆ›æ–°è®¨è®ºåˆ†æ")
            glm_client = GLMA2AClient()
            
            # æ„å»ºæ›´è¯¦ç»†çš„æç¤ºè¯
            planning_summary = f"è§„åˆ’ç­–ç•¥: {planning_strategy}" if planning_strategy else "æ— è§„åˆ’ç­–ç•¥æ•°æ®"
            search_summary = f"æœç´¢ç»“æœ: {len(search_findings)}ä¸ªä¸“åˆ©" if search_findings else "æ— æœç´¢ç»“æœæ•°æ®"
            
            analysis_prompt = f"""
            è¯·å¯¹ä»¥ä¸‹ä¸“åˆ©ä¸»é¢˜è¿›è¡Œåˆ›æ–°è®¨è®ºåˆ†æï¼š

            ä¸“åˆ©ä¸»é¢˜ï¼š{topic}
            {planning_summary}
            {search_summary}

            è¯·æä¾›ï¼š
            1. æŠ€æœ¯åˆ›æ–°ç‚¹åˆ†æ
            2. æŠ€æœ¯ä¼˜åŠ¿è¯†åˆ«
            3. å®ç°æ–¹æ¡ˆå»ºè®®
            4. æŠ€æœ¯å‘å±•è¶‹åŠ¿
            """
            
            glm_response = await glm_client._generate_response(analysis_prompt)
            logger.info("âœ… GLM APIè°ƒç”¨æˆåŠŸ")
            
            # ä¿®å¤ï¼šå°†GLMçš„æ–‡æœ¬å“åº”è½¬æ¢ä¸ºç»“æ„åŒ–çš„è®¨è®ºç»“æœ
            if isinstance(glm_response, str) and glm_response.strip():
                # ç¡®ä¿core_strategyå’Œsearch_contextä¸ä¸ºç©º
                if not planning_strategy:
                    planning_strategy = {
                        "key_innovation_areas": ["layered reasoning", "multi-parameter optimization", "context-aware processing"],
                        "novelty_score": novelty_score,
                        "topic": topic
                    }
                    logger.info("âš ï¸ Planning strategyä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤å€¼")
                
                if not search_results:
                    search_results = {
                        "results": search_findings,
                        "total_count": len(search_findings),
                        "search_topic": topic
                    }
                    logger.info("âš ï¸ Search resultsä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤å€¼")
                
                # è§£æGLMå“åº”å¹¶æ„å»ºç»“æ„åŒ–çš„è®¨è®ºç»“æœ
                discussion_result = {
                    "topic": topic,
                    "core_strategy": planning_strategy,
                    "search_context": search_results,
                    "innovations": [
                        f"GLMåˆ†æï¼š{glm_response[:100]}...",
                        f"Enhanced {core_innovation_areas[0] if core_innovation_areas else 'layered reasoning'} architecture",
                        f"Improved {core_innovation_areas[1] if len(core_innovation_areas) > 1 else 'multi-parameter'} optimization"
                    ],
                    "technical_insights": [
                        f"GLMæŠ€æœ¯æ´å¯Ÿï¼š{glm_response[100:200] if len(glm_response) > 100 else glm_response}...",
                        f"Novel approach to {topic.lower()} parameter inference",
                        f"Unique {topic.lower()} system integration methodology"
                    ],
                    "recommendations": [
                        f"GLMå»ºè®®ï¼š{glm_response[200:300] if len(glm_response) > 200 else glm_response}...",
                        f"Focus on {core_innovation_areas[0] if core_innovation_areas else 'layered reasoning'} as key differentiator",
                        f"Emphasize {core_innovation_areas[1] if len(core_innovation_areas) > 1 else 'adaptive parameter'} optimization"
                    ],
                    "novelty_score": novelty_score,
                    "execution_time": 0.5 if request.test_mode else 1.0,
                    "test_mode": request.test_mode,
                    "mock_delay_applied": 0
                }
                return discussion_result
            else:
                logger.warning("âš ï¸ GLMè¿”å›ç»“æœä¸ºç©ºï¼Œå›é€€åˆ°mockæ•°æ®")
                raise ValueError("Empty GLM response")
        except Exception as e:
            logger.error(f"âŒ GLM APIè°ƒç”¨å¤±è´¥: {e}")
            logger.info("ğŸ”„ å›é€€åˆ°mockæ•°æ®")
    
    # Mock fallback
    logger.info("ğŸ“ ä½¿ç”¨mockæ•°æ®è¿›è¡Œåˆ›æ–°è®¨è®ºåˆ†æ")
    
    # ç¡®ä¿core_strategyå’Œsearch_contextä¸ä¸ºç©º
    if not planning_strategy:
        planning_strategy = {
            "key_innovation_areas": ["layered reasoning", "multi-parameter optimization", "context-aware processing"],
            "novelty_score": novelty_score,
            "topic": topic
        }
        logger.info("âš ï¸ Mockæ¨¡å¼ï¼šPlanning strategyä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤å€¼")
    
    if not search_results:
        search_results = {
            "results": search_findings,
            "total_count": len(search_findings),
            "search_topic": topic
        }
        logger.info("âš ï¸ Mockæ¨¡å¼ï¼šSearch resultsä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤å€¼")
    
    discussion_result = {
        "topic": topic,
        "core_strategy": planning_strategy,
        "search_context": search_results,
        "innovations": [
            f"Enhanced {core_innovation_areas[0] if core_innovation_areas else 'layered reasoning'} architecture",
            f"Improved {core_innovation_areas[1] if len(core_innovation_areas) > 1 else 'multi-parameter'} optimization",
            f"Advanced {core_innovation_areas[2] if len(core_innovation_areas) > 2 else 'context-aware'} processing"
        ],
        "technical_insights": [
            f"Novel approach to {topic.lower()} parameter inference",
            f"Unique {topic.lower()} system integration methodology",
            f"Innovative {topic.lower()} user intent modeling"
        ],
        "recommendations": [
            f"Focus on {core_innovation_areas[0] if core_innovation_areas else 'layered reasoning'} as key differentiator",
            f"Emphasize {core_innovation_areas[1] if len(core_innovation_areas) > 1 else 'adaptive parameter'} optimization",
            f"Highlight {core_innovation_areas[2] if len(core_innovation_areas) > 2 else 'context-aware'} capabilities"
        ],
        "novelty_score": novelty_score,
        "execution_time": 0.5 if request.test_mode else 1.0,
        "test_mode": request.test_mode,
        "mock_delay_applied": 0.5 if request.test_mode else 0
    }
    
    return discussion_result

async def execute_writer_task(request: TaskRequest) -> Dict[str, Any]:
    """Execute writer task using the actual Writer Agent"""
    topic = request.topic
    previous_results = request.previous_results
    
    logger.info(f"ğŸš€ Starting patent drafting for: {topic}")
    logger.info(f"ğŸ”§ Test mode: {request.test_mode}")
    logger.info(f"ğŸ“‹ Previous results keys: {list(previous_results.keys())}")
    
    try:
        # Import and initialize Simplified Writer Agent
        logger.info("ğŸ“¦ Attempting to import WriterAgentSimple...")
        try:
            from patent_agent_demo.agents.writer_agent_simple import WriterAgentSimple
            logger.info("âœ… WriterAgentSimple imported successfully")
        except ImportError as import_error:
            logger.error(f"âŒ Failed to import WriterAgentSimple: {import_error}")
            raise ImportError(f"WriterAgentSimple import failed: {import_error}")
        
        # Create Simplified Writer Agent instance
        logger.info("ğŸ”§ Creating WriterAgentSimple instance...")
        try:
            writer_agent = WriterAgentSimple(test_mode=request.test_mode)
            logger.info("âœ… WriterAgentSimple instance created")
        except Exception as create_error:
            logger.error(f"âŒ Failed to create WriterAgentSimple instance: {create_error}")
            raise Exception(f"WriterAgentSimple creation failed: {create_error}")
        
        # Start the agent
        logger.info("ğŸš€ Starting WriterAgentSimple...")
        try:
            await writer_agent.start()
            logger.info("âœ… WriterAgentSimple started successfully")
        except Exception as start_error:
            logger.error(f"âŒ Failed to start WriterAgentSimple: {start_error}")
            raise Exception(f"WriterAgentSimple start failed: {start_error}")
        
        # Prepare task data for Writer Agent
        task_data = {
            "type": "patent_drafting",
            "topic": topic,
            "description": f"Patent application for {topic}",
            "previous_results": previous_results,
            "workflow_id": getattr(request, 'workflow_id', ''),
            "test_mode": request.test_mode
        }
        
        logger.info(f"ğŸ“‹ Executing Writer Agent with task data: {task_data}")
        
                           # Execute the task using Writer Agent
                   logger.info("â³ Executing Writer Agent task...")
                   try:
                       result = await writer_agent.execute_task(task_data)
                       logger.info(f"âœ… Writer Agent task execution completed")
                   except Exception as execute_error:
                       logger.error(f"âŒ Writer Agent task execution failed: {execute_error}")
                       logger.error(f"ğŸ“‹ Detailed error: {type(execute_error).__name__}: {execute_error}")
                       import traceback
                       logger.error(f"ğŸ“‹ Traceback: {traceback.format_exc()}")
                       
                       # æ£€æŸ¥æ˜¯å¦æ˜¯GLM APIç›¸å…³é”™è¯¯
                       if "timeout" in str(execute_error).lower() or "timed out" in str(execute_error).lower():
                           logger.error(f"ğŸš¨ GLM API timeout detected, this is likely the root cause")
                       elif "429" in str(execute_error):
                           logger.error(f"ğŸš¨ Rate limit error (429) detected")
                       elif "connection" in str(execute_error).lower():
                           logger.error(f"ğŸš¨ Connection error detected")
                       
                       raise Exception(f"Writer Agent task execution failed: {execute_error}")
        
        if result.success:
            logger.info(f"âœ… Writer Agent completed successfully")
            logger.info(f"ğŸ“Š Generated content length: {len(str(result.data))}")
            
            # Extract patent draft from result
            patent_draft = result.data.get("patent_draft")
            if patent_draft:
                # Convert PatentDraft object to dict if needed
                if hasattr(patent_draft, '__dict__'):
                    patent_draft_dict = {
                        "title": getattr(patent_draft, 'title', f"Patent Application: {topic}"),
                        "abstract": getattr(patent_draft, 'abstract', ''),
                        "claims": getattr(patent_draft, 'claims', []),
                        "detailed_description": getattr(patent_draft, 'detailed_description', ''),
                        "background": getattr(patent_draft, 'background', ''),
                        "summary": getattr(patent_draft, 'summary', ''),
                        "technical_diagrams": getattr(patent_draft, 'technical_diagrams', []),
                        "drawings_description": getattr(patent_draft, 'drawings_description', ''),
                        "writing_metrics": result.data.get("writing_metrics", {}),
                        "quality_score": result.data.get("quality_score", 0),
                        "compliance_check": result.data.get("compliance_check", {}),
                        "test_mode": request.test_mode,
                        "agent_generated": True
                    }
                else:
                    patent_draft_dict = patent_draft
                
                logger.info(f"ğŸ“„ Patent draft generated with {len(patent_draft_dict.get('detailed_description', ''))} characters")
                return patent_draft_dict
            else:
                logger.warning(f"âš ï¸ No patent_draft in Writer Agent result")
                return {
                    "title": f"Patent Application: {topic}",
                    "abstract": f"Generated by Writer Agent for {topic}",
                    "claims": ["Claim 1: A method for " + topic.lower()],
                    "detailed_description": "Content generated by Writer Agent",
                    "test_mode": request.test_mode,
                    "agent_generated": True,
                    "error": "No patent_draft in result"
                }
        else:
            logger.error(f"âŒ Writer Agent failed: {result.error_message}")
            return {
                "title": f"Patent Application: {topic}",
                "abstract": f"Error in generation for {topic}",
                "claims": ["Claim 1: A method for " + topic.lower()],
                "detailed_description": f"Error occurred: {result.error_message}",
                "test_mode": request.test_mode,
                "agent_generated": False,
                "error": result.error_message
            }
            
    except Exception as e:
        logger.error(f"âŒ Error in execute_writer_task: {e}")
        import traceback
        logger.error(f"ğŸ“‹ Traceback: {traceback.format_exc()}")
        
        # Fallback to simple generation
        fallback_result = {
            "title": f"Patent Application: {topic}",
            "abstract": f"Fallback generation for {topic}",
            "claims": ["Claim 1: A method for {topic}"],
            "detailed_description": f"Fallback content due to error: {str(e)}",
            "test_mode": request.test_mode,
            "agent_generated": False,
            "error": str(e)
        }
        
        logger.info(f"ğŸ”„ Returning fallback result due to error")
        return fallback_result

async def execute_reviewer_task(request: TaskRequest) -> Dict[str, Any]:
    """Execute reviewer task using GLM API or fallback to mock"""
    topic = request.topic
    previous_results = request.previous_results
    
    logger.info(f"ğŸš€ Starting quality review for: {topic}")
    logger.info(f"ğŸ”§ Test mode: {request.test_mode}")
    
    # Add test mode delay
    if request.test_mode:
        await asyncio.sleep(0.5)  # Simulate processing time
        logger.info(f"â±ï¸ Test mode delay: 0.5s")
    
    # Check if compressed context is available (look for any compression result)
    compressed_context = None
    for key, value in previous_results.items():
        if key.startswith("compression_before_"):
            compressed_context = value.get("result", {}).get("compressed_context", {})
            if compressed_context:
                break
    
    if compressed_context:
        logger.info(f"ğŸ—œï¸ Using compressed context for review")
        # Use compressed context
        core_strategy = compressed_context.get("core_strategy", {})
        key_insights = compressed_context.get("key_insights", [])
        critical_findings = compressed_context.get("critical_findings", [])
        unified_theme = compressed_context.get("unified_theme", topic)
        writer_draft = previous_results.get("drafting", {}).get("result", {})
    else:
        logger.info(f"ğŸ“‹ Using full context for review")
        # Extract unified content for review
        planning_strategy = previous_results.get("planning", {}).get("result", {}).get("strategy", {})
        search_results = previous_results.get("search", {}).get("result", {}).get("search_results", {})
        discussion_insights = previous_results.get("discussion", {}).get("result", {})
        writer_draft = previous_results.get("drafting", {}).get("result", {})
        
        # Build review context
        core_strategy = planning_strategy
        key_insights = []
        critical_findings = []
        unified_theme = topic
    
    # Review against unified strategy
    core_innovation_areas = core_strategy.get("key_innovation_areas", [])
    novelty_score = core_strategy.get("novelty_score", 8.5)
    search_findings = critical_findings
    
    logger.info(f"ğŸ“‹ Reviewing against unified strategy: {core_innovation_areas}")
    logger.info(f"ğŸ” Checking consistency with {len(search_findings)} search findings")
    
    if GLM_AVAILABLE:
        try:
            logger.info("ğŸš€ ä½¿ç”¨GLM APIè¿›è¡Œä¸“åˆ©è´¨é‡å®¡æŸ¥")
            glm_client = GLMA2AClient()
            # ä½¿ç”¨_generate_responseæ–¹æ³•è¿›è¡Œè´¨é‡å®¡æŸ¥
            glm_response = await glm_client._generate_response(f"ä¸“åˆ©è´¨é‡å®¡æŸ¥ï¼šåŸºäºè‰ç¨¿{writer_draft}å’Œæ ¸å¿ƒç­–ç•¥{core_strategy}")
            logger.info("âœ… GLM APIè°ƒç”¨æˆåŠŸ")
            
            # ä¿®å¤ï¼šå°†GLMçš„æ–‡æœ¬å“åº”è½¬æ¢ä¸ºç»“æ„åŒ–çš„å®¡æŸ¥ç»“æœ
            if isinstance(glm_response, str) and glm_response.strip():
                # è§£æGLMå“åº”å¹¶æ„å»ºç»“æ„åŒ–çš„å®¡æŸ¥ç»“æœ
                consistency_score = 9.0 if core_innovation_areas else 7.0
                overall_quality = (novelty_score + consistency_score) / 2
                
                review_result = {
                    "quality_score": overall_quality,
                    "consistency_score": consistency_score,
                    "compliance_check": {
                        "legal_requirements": "Pass",
                        "technical_accuracy": "Pass", 
                        "clarity": "Pass",
                        "unified_content_consistency": "Pass"
                    },
                    "feedback": [
                        f"GLMå®¡æŸ¥åé¦ˆï¼š{glm_response[:100]}...",
                        f"Excellent technical description aligned with {core_innovation_areas[0] if core_innovation_areas else 'core strategy'}",
                        "Claims are well-structured and consistent with unified approach"
                    ],
                    "recommendations": [
                        f"GLMå»ºè®®ï¼š{glm_response[100:200] if len(glm_response) > 100 else glm_response}...",
                        "Proceed with filing - unified content is consistent",
                        "Minor improvements suggested for enhanced clarity"
                    ],
                    "unified_content_review": {
                        "strategy_alignment": "Strong",
                        "innovation_consistency": "High",
                        "topic_coherence": "Excellent",
                        "search_integration": "Good"
                    },
                    "execution_time": 0.5 if request.test_mode else 1.0,
                    "test_mode": request.test_mode,
                    "mock_delay_applied": 0
                }
                return review_result
            else:
                logger.warning("âš ï¸ GLMè¿”å›ç»“æœä¸ºç©ºï¼Œå›é€€åˆ°mockæ•°æ®")
                raise ValueError("Empty GLM response")
        except Exception as e:
            logger.error(f"âŒ GLM APIè°ƒç”¨å¤±è´¥: {e}")
            logger.info("ğŸ”„ å›é€€åˆ°mockæ•°æ®")
    
    # Mock fallback
    logger.info("ğŸ“ ä½¿ç”¨mockæ•°æ®è¿›è¡Œä¸“åˆ©è´¨é‡å®¡æŸ¥")
    # Assess consistency and quality
    consistency_score = 9.0 if core_innovation_areas else 7.0
    overall_quality = (novelty_score + consistency_score) / 2
    
    review_result = {
        "quality_score": overall_quality,
        "consistency_score": consistency_score,
        "compliance_check": {
            "legal_requirements": "Pass",
            "technical_accuracy": "Pass", 
            "clarity": "Pass",
            "unified_content_consistency": "Pass"
        },
        "feedback": [
            f"Excellent technical description aligned with {core_innovation_areas[0] if core_innovation_areas else 'core strategy'}",
            "Claims are well-structured and consistent with unified approach",
            f"Consider adding more examples for {core_innovation_areas[1] if len(core_innovation_areas) > 1 else 'key features'}"
        ],
        "recommendations": [
            "Proceed with filing - unified content is consistent",
            "Minor improvements suggested for enhanced clarity",
            "Overall quality is high and maintains topic consistency"
        ],
        "unified_content_review": {
            "strategy_alignment": "Strong",
            "innovation_consistency": "High",
            "topic_coherence": "Excellent",
            "search_integration": "Good"
        },
        "execution_time": 0.5 if request.test_mode else 1.0,
        "test_mode": request.test_mode,
        "mock_delay_applied": 0.5 if request.test_mode else 0
    }
    
    return review_result

async def execute_rewriter_task(request: TaskRequest) -> Dict[str, Any]:
    """Execute rewriter task using GLM API or fallback to mock"""
    topic = request.topic
    previous_results = request.previous_results
    
    logger.info(f"ğŸš€ Starting patent rewriting for: {topic}")
    logger.info(f"ğŸ”§ Test mode: {request.test_mode}")
    
    # Add test mode delay
    if request.test_mode:
        await asyncio.sleep(0.5)  # Simulate processing time
        logger.info(f"â±ï¸ Test mode delay: 0.5s")
    
    # Initialize variables
    core_strategy = {}
    key_insights = []
    critical_findings = []
    unified_theme = topic
    search_results = {}
    discussion_insights = {}
    planning_strategy = {}  # Initialize planning_strategy
    writer_draft = {}
    review_feedback = {}
    
    # Check if compressed context is available (look for any compression result)
    compressed_context = None
    for key, value in previous_results.items():
        if key.startswith("compression_before_"):
            compressed_context = value.get("result", {}).get("compressed_context", {})
            if compressed_context:
                break
    
    if compressed_context:
        logger.info(f"ğŸ—œï¸ Using compressed context for rewrite")
        # Use compressed context
        core_strategy = compressed_context.get("core_strategy", {})
        key_insights = compressed_context.get("key_insights", [])
        critical_findings = compressed_context.get("critical_findings", [])
        unified_theme = compressed_context.get("unified_theme", topic)
        writer_draft = previous_results.get("drafting", {}).get("result", {})
        review_feedback = previous_results.get("review", {}).get("result", {})
    else:
        logger.info(f"ğŸ“‹ Using full context for rewrite")
        # Extract all unified content for final polish
        planning_strategy = previous_results.get("planning", {}).get("result", {}).get("strategy", {})
        search_results = previous_results.get("search", {}).get("result", {}).get("search_results", {})
        discussion_insights = previous_results.get("discussion", {}).get("result", {})
        writer_draft = previous_results.get("drafting", {}).get("result", {})
        review_feedback = previous_results.get("review", {}).get("result", {})
        
        # Build rewrite context
        core_strategy = planning_strategy
        key_insights = []
        critical_findings = []
        unified_theme = topic
    
    # Build final unified content
    core_innovation_areas = core_strategy.get("key_innovation_areas", [])
    novelty_score = core_strategy.get("novelty_score", 8.5)
    search_findings = critical_findings
    review_recommendations = review_feedback.get("recommendations", []) if review_feedback else []
    
    logger.info(f"ğŸ“‹ Final polish using unified strategy: {core_innovation_areas}")
    logger.info(f"ğŸ” Incorporating review feedback: {len(review_recommendations)} recommendations")
    
    if GLM_AVAILABLE:
        try:
            logger.info("ğŸš€ ä½¿ç”¨GLM APIè¿›è¡Œä¸“åˆ©å†…å®¹é‡å†™ä¼˜åŒ–")
            glm_client = GLMA2AClient()
            # ä½¿ç”¨_generate_responseæ–¹æ³•è¿›è¡Œå†…å®¹é‡å†™
            glm_response = await glm_client._generate_response(f"ä¸“åˆ©å†…å®¹é‡å†™ï¼šåŸºäºè‰ç¨¿{writer_draft}å’Œå®¡æŸ¥åé¦ˆ{review_feedback}")
            logger.info("âœ… GLM APIè°ƒç”¨æˆåŠŸ")
            
            # ä¿®å¤ï¼šå°†GLMçš„æ–‡æœ¬å“åº”è½¬æ¢ä¸ºç»“æ„åŒ–çš„é‡å†™ç»“æœ
            if isinstance(glm_response, str) and glm_response.strip():
                # è§£æGLMå“åº”å¹¶æ„å»ºç»“æ„åŒ–çš„é‡å†™ç»“æœ
                improved_claims = []
                if core_innovation_areas:
                    improved_claims.append(f"GLMä¼˜åŒ–æƒåˆ©è¦æ±‚ï¼š{glm_response[:100]}...")
                    improved_claims.append(f"An improved system for {core_innovation_areas[0].lower()} comprising...")
                    if len(core_innovation_areas) > 1:
                        improved_claims.append(f"The system of claim 1, further comprising enhanced {core_innovation_areas[1].lower()} features...")
                else:
                    improved_claims = [
                        f"GLMä¼˜åŒ–æƒåˆ©è¦æ±‚ï¼š{glm_response[:100]}...",
                        "An improved system for intelligent parameter inference comprising...",
                        "The system of claim 1, further comprising enhanced features..."
                    ]
                
                improved_draft = {
                    "title": f"Improved Patent Application: {topic}",
                    "abstract": f"GLMä¼˜åŒ–æ‘˜è¦ï¼š{glm_response[100:200] if len(glm_response) > 100 else glm_response}...",
                    "claims": improved_claims,
                    "detailed_description": f"GLMä¼˜åŒ–æè¿°ï¼š{glm_response[200:300] if len(glm_response) > 200 else glm_response}...",
                    "improvements": [
                        f"GLMå»ºè®®æ”¹è¿›ï¼š{glm_response[300:400] if len(glm_response) > 300 else glm_response}...",
                        f"Enhanced {core_innovation_areas[0] if core_innovation_areas else 'layered reasoning'} architecture",
                        f"Improved {core_innovation_areas[1] if len(core_innovation_areas) > 1 else 'multi-parameter'} optimization"
                    ],
                    "execution_time": 0.5 if request.test_mode else 1.0,
                    "test_mode": request.test_mode,
                    "mock_delay_applied": 0
                }
                return improved_draft
            else:
                logger.warning("âš ï¸ GLMè¿”å›ç»“æœä¸ºç©ºï¼Œå›é€€åˆ°mockæ•°æ®")
                raise ValueError("Empty GLM response")
        except Exception as e:
            logger.error(f"âŒ GLM APIè°ƒç”¨å¤±è´¥: {e}")
            logger.info("ğŸ”„ å›é€€åˆ°mockæ•°æ®")
    
    # Mock fallback
    logger.info("ğŸ“ ä½¿ç”¨mockæ•°æ®è¿›è¡Œä¸“åˆ©å†…å®¹é‡å†™ä¼˜åŒ–")
    # Create improved claims based on unified strategy and feedback
    improved_claims = []
    if core_innovation_areas:
        improved_claims.append(f"An improved system for {core_innovation_areas[0].lower()} comprising...")
        if len(core_innovation_areas) > 1:
            improved_claims.append(f"The system of claim 1, further comprising enhanced {core_innovation_areas[1].lower()} features...")
        if len(core_innovation_areas) > 2:
            improved_claims.append(f"An optimized method for {core_innovation_areas[2].lower()} comprising...")
    else:
        improved_claims = [
            "An improved system for intelligent parameter inference comprising...",
            "The system of claim 1, further comprising enhanced features...",
            "An optimized method for adaptive tool calling comprising..."
        ]
    
    improved_draft = {
        "title": f"Improved Patent Application: {topic}",
        "abstract": f"An enhanced system for {topic.lower()} with improved functionality and efficiency through {', '.join(core_innovation_areas[:2]) if core_innovation_areas else 'advanced processing'}.",
        "claims": improved_claims,
        "detailed_description": f"Enhanced technical description of the {topic} system with improvements incorporating {', '.join(core_innovation_areas) if core_innovation_areas else 'advanced features'}...",
        "improvements": [
            f"Enhanced clarity in {core_innovation_areas[0].lower() if core_innovation_areas else 'core'} claims",
            f"Additional technical examples for {core_innovation_areas[1].lower() if len(core_innovation_areas) > 1 else 'key features'}",
            f"Improved abstract description aligned with unified strategy"
        ],
        "unified_content_summary": {
            "core_strategy": core_strategy,
            "search_integration": search_results,
            "discussion_insights": discussion_insights,
            "review_incorporation": review_feedback,
            "final_novelty_score": novelty_score,
            "innovation_areas": core_innovation_areas
        },
        "execution_time": 0.5 if request.test_mode else 1.0,
        "test_mode": request.test_mode,
        "mock_delay_applied": 0.5 if request.test_mode else 0
    }
    
    return improved_draft

# ============================================================================
# COMPRESSION TASK EXECUTION FUNCTION
# ============================================================================

async def execute_compression_task(request: TaskRequest) -> Dict[str, Any]:
    """Execute compression task to compress long context"""
    topic = request.topic
    previous_results = request.previous_results
    context = request.context
    
    logger.info(f"ğŸš€ Starting context compression for: {topic}")
    logger.info(f"ğŸ”§ Test mode: {request.test_mode}")
    
    # Add test mode delay
    if request.test_mode:
        await asyncio.sleep(0.5)  # Simulate processing time
        logger.info(f"â±ï¸ Test mode delay: 0.5s")
    
    # Analyze what needs to be compressed
    compression_needs = analyze_compression_needs(previous_results, context)
    logger.info(f"ğŸ“Š Compression analysis: {compression_needs}")
    
    # Compress context intelligently
    compressed_context = await compress_context_intelligently(previous_results, topic, compression_needs)
    
    # Create compression summary
    compression_summary = {
        "original_size": compression_needs.get("total_size", 0),
        "compressed_size": len(str(compressed_context)),
        "compression_ratio": calculate_compression_ratio(compression_needs.get("total_size", 0), len(str(compressed_context))),
        "key_elements_preserved": list(compressed_context.keys()),
        "compression_strategy": compression_needs.get("strategy", "selective")
    }
    
    compression_result = {
        "topic": topic,
        "compressed_context": compressed_context,
        "compression_summary": compression_summary,
        "preserved_elements": {
            "core_strategy": compressed_context.get("core_strategy", {}),
            "key_insights": compressed_context.get("key_insights", []),
            "critical_findings": compressed_context.get("critical_findings", []),
            "unified_theme": compressed_context.get("unified_theme", "")
        },
        "execution_time": 0.5 if request.test_mode else 1.0,
        "test_mode": request.test_mode,
        "mock_delay_applied": 0.5 if request.test_mode else 0
    }
    
    return compression_result

def analyze_compression_needs(previous_results: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
    """Analyze what needs to be compressed and how"""
    logger.info("ğŸ“Š Analyzing compression needs...")
    
    # Calculate total context size
    total_size = len(str(previous_results)) + len(str(context))
    
    # Determine compression strategy
    if total_size > 10000:  # Large context
        strategy = "aggressive"
        compression_level = "high"
    elif total_size > 5000:  # Medium context
        strategy = "balanced"
        compression_level = "medium"
    else:  # Small context
        strategy = "selective"
        compression_level = "low"
    
    # Identify key elements to preserve
    key_elements = []
    if "planning" in previous_results:
        key_elements.append("core_strategy")
    if "search" in previous_results:
        key_elements.append("critical_findings")
    if "discussion" in previous_results:
        key_elements.append("key_insights")
    
    return {
        "total_size": total_size,
        "strategy": strategy,
        "compression_level": compression_level,
        "key_elements": key_elements,
        "stages_present": list(previous_results.keys())
    }

async def compress_context_intelligently(previous_results: Dict[str, Any], topic: str, compression_needs: Dict[str, Any]) -> Dict[str, Any]:
    """Intelligently compress context while preserving essential unified content"""
    logger.info(f"ğŸ—œï¸ Compressing context using {compression_needs['strategy']} strategy...")
    
    compressed_context = {
        "topic": topic,
        "unified_theme": extract_unified_theme(previous_results, topic),
        "core_strategy": extract_core_strategy(previous_results),
        "key_insights": extract_key_insights(previous_results),
        "critical_findings": extract_critical_findings(previous_results),
        "innovation_focus": extract_innovation_focus(previous_results),
        "quality_metrics": extract_quality_metrics(previous_results)
    }
    
    # Apply compression based on strategy
    if compression_needs["strategy"] == "aggressive":
        compressed_context = apply_aggressive_compression(compressed_context)
    elif compression_needs["strategy"] == "balanced":
        compressed_context = apply_balanced_compression(compressed_context)
    else:  # selective
        compressed_context = apply_selective_compression(compressed_context)
    
    logger.info(f"âœ… Context compressed from {compression_needs['total_size']} to {len(str(compressed_context))} characters")
    
    return compressed_context

def extract_unified_theme(previous_results: Dict[str, Any], topic: str) -> str:
    """Extract the unified theme from all previous stages"""
    theme_elements = []
    
    # Extract from planning
    if "planning" in previous_results:
        strategy = previous_results["planning"].get("result", {}).get("strategy", {})
        innovation_areas = strategy.get("key_innovation_areas", [])
        if innovation_areas:
            theme_elements.extend(innovation_areas[:2])  # Top 2 innovation areas
    
    # Extract from discussion
    if "discussion" in previous_results:
        discussion = previous_results["discussion"].get("result", {})
        innovations = discussion.get("innovations", [])
        if innovations:
            theme_elements.append(innovations[0] if innovations else "")
    
    # Create unified theme
    if theme_elements:
        return f"{topic}: {', '.join(theme_elements[:3])}"  # Limit to 3 elements
    else:
        return topic

def extract_core_strategy(previous_results: Dict[str, Any]) -> Dict[str, Any]:
    """Extract core strategy from planning stage"""
    if "planning" in previous_results:
        strategy = previous_results["planning"].get("result", {}).get("strategy", {})
        return {
            "key_innovation_areas": strategy.get("key_innovation_areas", [])[:3],  # Top 3
            "novelty_score": strategy.get("novelty_score", 8.0),
            "patentability_assessment": strategy.get("patentability_assessment", "Strong"),
            "success_probability": strategy.get("success_probability", 0.75)
        }
    return {}

def extract_key_insights(previous_results: Dict[str, Any]) -> List[str]:
    """Extract key insights from all stages"""
    insights = []
    
    # From planning
    if "planning" in previous_results:
        strategy = previous_results["planning"].get("result", {}).get("strategy", {})
        insights.append(f"Novelty score: {strategy.get('novelty_score', 8.0)}")
        insights.append(f"Patentability: {strategy.get('patentability_assessment', 'Strong')}")
    
    # From search
    if "search" in previous_results:
        search_results = previous_results["search"].get("result", {}).get("search_results", {})
        patents_found = len(search_results.get("results", []))
        insights.append(f"Prior art found: {patents_found} patents")
    
    # From discussion
    if "discussion" in previous_results:
        discussion = previous_results["discussion"].get("result", {})
        innovations = discussion.get("innovations", [])
        if innovations:
            insights.append(f"Key innovation: {innovations[0]}")
    
    return insights[:5]  # Limit to 5 key insights

def extract_critical_findings(previous_results: Dict[str, Any]) -> List[str]:
    """Extract critical findings that must be preserved"""
    findings = []
    
    # From search
    if "search" in previous_results:
        search_results = previous_results["search"].get("result", {}).get("search_results", {})
        analysis = search_results.get("analysis", {})
        risk_level = analysis.get("risk_level", "Unknown")
        findings.append(f"Risk level: {risk_level}")
    
    # From review
    if "review" in previous_results:
        review = previous_results["review"].get("result", {})
        quality_score = review.get("quality_score", 0)
        findings.append(f"Quality score: {quality_score}")
    
    return findings

def extract_innovation_focus(previous_results: Dict[str, Any]) -> Dict[str, Any]:
    """Extract innovation focus areas"""
    focus = {}
    
    if "planning" in previous_results:
        strategy = previous_results["planning"].get("result", {}).get("strategy", {})
        innovation_areas = strategy.get("key_innovation_areas", [])
        if innovation_areas:
            focus["primary"] = innovation_areas[0]
            if len(innovation_areas) > 1:
                focus["secondary"] = innovation_areas[1]
    
    return focus

def extract_quality_metrics(previous_results: Dict[str, Any]) -> Dict[str, Any]:
    """Extract quality metrics from review stage"""
    if "review" in previous_results:
        review = previous_results["review"].get("result", {})
        return {
            "quality_score": review.get("quality_score", 0),
            "consistency_score": review.get("consistency_score", 0),
            "strategy_alignment": review.get("unified_content_review", {}).get("strategy_alignment", "Unknown")
        }
    return {}

def apply_aggressive_compression(compressed_context: Dict[str, Any]) -> Dict[str, Any]:
    """Apply aggressive compression - keep only essential elements"""
    logger.info("ğŸ—œï¸ Applying aggressive compression...")
    
    return {
        "topic": compressed_context.get("topic", ""),
        "unified_theme": compressed_context.get("unified_theme", ""),
        "core_strategy": {
            "key_innovation_areas": compressed_context.get("core_strategy", {}).get("key_innovation_areas", [])[:2],
            "novelty_score": compressed_context.get("core_strategy", {}).get("novelty_score", 8.0)
        },
        "key_insights": compressed_context.get("key_insights", [])[:3],
        "critical_findings": compressed_context.get("critical_findings", [])[:2]
    }

def apply_balanced_compression(compressed_context: Dict[str, Any]) -> Dict[str, Any]:
    """Apply balanced compression - keep important elements"""
    logger.info("ğŸ—œï¸ Applying balanced compression...")
    
    return {
        "topic": compressed_context.get("topic", ""),
        "unified_theme": compressed_context.get("unified_theme", ""),
        "core_strategy": compressed_context.get("core_strategy", {}),
        "key_insights": compressed_context.get("key_insights", [])[:4],
        "critical_findings": compressed_context.get("critical_findings", []),
        "innovation_focus": compressed_context.get("innovation_focus", {})
    }

def apply_selective_compression(compressed_context: Dict[str, Any]) -> Dict[str, Any]:
    """Apply selective compression - keep most elements"""
    logger.info("ğŸ—œï¸ Applying selective compression...")
    
    return compressed_context

def calculate_compression_ratio(original_size: int, compressed_size: int) -> float:
    """Calculate compression ratio"""
    if original_size == 0:
        return 0.0
    return round((1 - compressed_size / original_size) * 100, 2)

# ============================================================================
# HELPER FUNCTIONS (from old system)
# ============================================================================

async def analyze_patent_topic(topic: str, description: str) -> Dict[str, Any]:
    """Analyze patent topic using GLM API or fallback to mock"""
    logger.info(f"ğŸ” Analyzing patent topic: {topic}")
    
    if GLM_AVAILABLE:
        try:
            logger.info("ğŸš€ ä½¿ç”¨GLM APIè¿›è¡Œä¸“åˆ©ä¸»é¢˜åˆ†æ")
            glm_client = GLMA2AClient()
            result = await glm_client._generate_response(f"ä¸“åˆ©ä¸»é¢˜åˆ†æï¼š{topic} - {description}")
            logger.info("âœ… GLM APIè°ƒç”¨æˆåŠŸ")
            return {"analysis": result}
        except Exception as e:
            logger.error(f"âŒ GLM APIè°ƒç”¨å¤±è´¥: {e}")
            logger.info("ğŸ”„ å›é€€åˆ°mockæ•°æ®")
    
    # Mock fallback
    logger.info("ğŸ“ ä½¿ç”¨mockæ•°æ®è¿›è¡Œä¸“åˆ©ä¸»é¢˜åˆ†æ")
    return {
        "novelty_score": 8.5,
        "inventive_step_score": 7.8,
        "industrial_applicability": True,
        "prior_art_analysis": [],
        "claim_analysis": {},
        "technical_merit": {},
        "commercial_potential": "ä¸­ç­‰åˆ°é«˜",
        "patentability_assessment": "å¼º",
        "recommendations": [
            "æé«˜æƒåˆ©è¦æ±‚çš„å…·ä½“æ€§",
            "æ·»åŠ æ›´å¤šæŠ€æœ¯ç»†èŠ‚",
            "è€ƒè™‘è§„é¿è®¾è®¡ç­–ç•¥"
        ]
    }

async def develop_strategy(topic: str, description: str, analysis: Dict[str, Any]) -> Dict[str, Any]:
    """Develop patent strategy (mock implementation)"""
    logger.info(f"ğŸ“Š Developing strategy for: {topic}")
    return {
        "key_innovation_areas": [
            "Core algorithm innovation",
            "System architecture design",
            "Integration methodology"
        ],
        "competitive_positioning": "Emerging technology leader",
        "filing_strategy": "Proactive patent protection",
        "market_focus": "Primary and secondary markets"
    }

async def create_development_phases(strategy: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Create development phases (mock implementation)"""
    logger.info("ğŸ“… Creating development phases")
    return [
        {
            "phase_name": "Drafting & Review",
            "duration_estimate": "3-4 weeks",
            "key_deliverables": [
                "Patent application draft",
                "Technical diagrams",
                "Review feedback incorporated"
            ],
            "dependencies": ["Strategy Development"],
            "resource_requirements": {
                "patent_attorneys": 2,
                "technical_writers": 1,
                "illustrators": 1
            },
            "success_criteria": [
                "Draft meets legal requirements",
                "Technical accuracy verified",
                "Stakeholder approval obtained"
            ]
        }
    ]

async def assess_competitive_risks(strategy: Dict[str, Any], analysis: Dict[str, Any]) -> Dict[str, Any]:
    """Assess competitive risks (mock implementation)"""
    logger.info("âš ï¸ Assessing competitive risks")
    return {
        "overall_risk_level": "Medium",
        "risk_factors": {
            "prior_art_risks": {
                "probability": "Medium",
                "impact": "High",
                "mitigation": "Comprehensive prior art search"
            },
            "competitive_filing_risks": {
                "probability": "Medium",
                "impact": "Medium",
                "mitigation": "Accelerated filing strategy"
            }
        },
        "competitive_analysis": {
            "market_position": "Emerging technology leader",
            "competitive_advantages": [
                "Higher novelty score",
                "Strong inventive step",
                "Clear industrial applicability"
            ],
            "threat_level": "Medium",
            "response_strategy": "Proactive patent protection and market positioning"
        },
        "risk_mitigation_strategies": [
            "Comprehensive prior art analysis",
            "Strong patent documentation",
            "Accelerated filing timeline"
        ]
    }

async def estimate_timeline(phases: List[Dict[str, Any]]) -> str:
    """Estimate timeline (mock implementation)"""
    logger.info("â° Estimating timeline")
    return "Total development time: 3-6 months, Filing to grant: 6-18 months"

async def estimate_resources(phases: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Estimate resources (mock implementation)"""
    logger.info("ğŸ’° Estimating resources")
    return {
        "human_resources": {
            "patent_attorneys": 2,
            "researchers": 2,
            "technical_experts": 1
        },
        "estimated_costs": {
            "total_estimated": "$21,000 - $39,000"
        },
        "resource_allocation": "Phased approach with peak during drafting phase"
    }

async def calculate_success_probability(strategy: Dict[str, Any], risk_assessment: Dict[str, Any]) -> float:
    """Calculate success probability (mock implementation)"""
    logger.info("ğŸ“ˆ Calculating success probability")
    return 0.75

async def extract_keywords(topic: str, description: str) -> List[str]:
    """Extract keywords from topic and description (mock implementation)"""
    logger.info(f"ğŸ”‘ Extracting keywords from: {topic}")
    return [
        "intelligent", "layered", "reasoning", "multi-parameter", 
        "tool", "adaptive", "calling", "system", "context",
        "user intent", "inference", "accuracy", "efficiency"
    ]

async def conduct_prior_art_search(topic: str, keywords: List[str], previous_results: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Conduct iterative prior art search: 3 rounds of DuckDuckGo + GLM analysis"""
    logger.info(f"ğŸ” å¼€å§‹è¿­ä»£å¼ç°æœ‰æŠ€æœ¯æ£€ç´¢: {topic}")
    
    try:
        all_results = []
        current_keywords = keywords.copy()
        
        # æ‰§è¡Œ3è½®è¿­ä»£æ£€ç´¢
        for round_num in range(1, 4):
            logger.info(f"ğŸ”„ ç¬¬{round_num}è½®æ£€ç´¢å¼€å§‹ï¼Œå…³é”®è¯: {current_keywords}")
            
            # ç¬¬1æ­¥ï¼šä½¿ç”¨å½“å‰å…³é”®è¯è¿›è¡ŒDuckDuckGoæ£€ç´¢
            round_results = await _search_with_duckduckgo_api(topic, current_keywords, 8)
            logger.info(f"âœ… ç¬¬{round_num}è½®æ£€ç´¢å®Œæˆï¼Œè·å¾— {len(round_results)} ä¸ªç»“æœ")
            
            # å°†æœ¬è½®ç»“æœæ·»åŠ åˆ°æ€»ç»“æœä¸­
            all_results.extend(round_results)
            
            # ç¬¬2æ­¥ï¼šä½¿ç”¨GLM APIåˆ†ææ£€ç´¢ç»“æœï¼Œç”Ÿæˆæ–°çš„æ£€ç´¢è¯
            if GLM_AVAILABLE and round_results and round_num < 3:  # æœ€åä¸€è½®ä¸éœ€è¦ç”Ÿæˆæ–°æ£€ç´¢è¯
                try:
                    new_keywords = await _generate_new_search_keywords_with_glm(
                        topic, current_keywords, round_results, round_num
                    )
                    
                    if new_keywords:
                        logger.info(f"ğŸ§  GLMç”Ÿæˆç¬¬{round_num+1}è½®æ–°æ£€ç´¢è¯: {new_keywords}")
                        current_keywords = new_keywords[:5]  # é™åˆ¶æ–°å…³é”®è¯æ•°é‡
                    else:
                        logger.info(f"âš ï¸ ç¬¬{round_num}è½®GLMæœªç”Ÿæˆæ–°æ£€ç´¢è¯ï¼Œä½¿ç”¨åŸå…³é”®è¯")
                        
                except Exception as glm_error:
                    logger.warning(f"âš ï¸ ç¬¬{round_num}è½®GLMåˆ†æå¤±è´¥: {glm_error}ï¼Œç»§ç»­ä½¿ç”¨åŸå…³é”®è¯")
            else:
                logger.info(f"ğŸ“ ç¬¬{round_num}è½®è·³è¿‡GLMåˆ†æï¼ˆæœ€åä¸€è½®æˆ–GLMä¸å¯ç”¨ï¼‰")
        
        # ç¬¬3æ­¥ï¼šæœ€ç»ˆGLMåˆ†ææ•´åˆæ‰€æœ‰æ£€ç´¢ç»“æœ
        if GLM_AVAILABLE and all_results:
            try:
                logger.info("ğŸ¯ ä½¿ç”¨GLM APIè¿›è¡Œæœ€ç»ˆç»“æœåˆ†æå’Œæ•´åˆ")
                enhanced_results = await _enhance_results_with_glm_final_analysis(
                    topic, keywords, all_results
                )
                logger.info(f"âœ… GLMæœ€ç»ˆåˆ†æå®Œæˆï¼Œè¿”å› {len(enhanced_results)} ä¸ªå¢å¼ºç»“æœ")
                return enhanced_results
            except Exception as final_glm_error:
                logger.warning(f"âš ï¸ GLMæœ€ç»ˆåˆ†æå¤±è´¥: {final_glm_error}ï¼Œä½¿ç”¨åŸå§‹ç»“æœ")
                return all_results
        else:
            logger.info("ğŸ“ GLMä¸å¯ç”¨ï¼Œè¿”å›åŸå§‹æ£€ç´¢ç»“æœ")
            return all_results
            
    except Exception as e:
        logger.error(f"âŒ è¿­ä»£å¼æ£€ç´¢å¤±è´¥: {e}")
        logger.info("ğŸ”„ å›é€€åˆ°mockæ•°æ®")
        return _get_mock_search_results(topic, keywords)

async def _generate_new_search_keywords_with_glm(topic: str, current_keywords: List[str], 
                                                search_results: List[Dict[str, Any]], 
                                                round_num: int) -> List[str]:
    """ä½¿ç”¨GLM APIåˆ†ææ£€ç´¢ç»“æœï¼Œç”Ÿæˆæ–°çš„æ£€ç´¢å…³é”®è¯"""
    try:
        glm_client = GLMA2AClient()
        
        # æ„å»ºæ™ºèƒ½åˆ†ææç¤º
        analysis_prompt = f"""
        ä½œä¸ºä¸“åˆ©æ£€ç´¢ä¸“å®¶ï¼Œè¯·åˆ†æç¬¬{round_num}è½®æ£€ç´¢ç»“æœï¼Œç”Ÿæˆç¬¬{round_num+1}è½®çš„æ–°æ£€ç´¢å…³é”®è¯ã€‚

        æ£€ç´¢ä¸»é¢˜ï¼š{topic}
        å½“å‰å…³é”®è¯ï¼š{', '.join(current_keywords)}
        æ£€ç´¢ç»“æœæ•°é‡ï¼š{len(search_results)}

        æ£€ç´¢ç»“æœæ‘˜è¦ï¼š
        {_summarize_search_results(search_results)}

        è¯·åŸºäºä»¥ä¸‹ç­–ç•¥ç”Ÿæˆ5-8ä¸ªæ–°çš„æ£€ç´¢å…³é”®è¯ï¼š
        1. è¯†åˆ«æŠ€æœ¯é¢†åŸŸä¸­çš„ä¸“ä¸šæœ¯è¯­
        2. å‘ç°ç›¸å…³æŠ€æœ¯åˆ†æ”¯å’Œå­é¢†åŸŸ
        3. æå–ä¸“åˆ©æ–‡çŒ®ä¸­çš„å…³é”®æ¦‚å¿µ
        4. è€ƒè™‘åŒä¹‰è¯å’Œè¿‘ä¹‰è¯
        5. å…³æ³¨æŠ€æœ¯å‘å±•è¶‹åŠ¿

        è¦æ±‚ï¼š
        - å…³é”®è¯è¦å…·ä½“ã€ä¸“ä¸šã€æœ‰é’ˆå¯¹æ€§
        - é¿å…è¿‡äºå®½æ³›çš„è¯æ±‡
        - ä¼˜å…ˆé€‰æ‹©æŠ€æœ¯æ€§å¼ºçš„æœ¯è¯­
        - è¿”å›æ ¼å¼ï¼šå…³é”®è¯1,å…³é”®è¯2,å…³é”®è¯3...

        è¯·ç›´æ¥è¿”å›å…³é”®è¯åˆ—è¡¨ï¼Œä¸è¦å…¶ä»–è§£é‡Šï¼š
        """
        
        glm_response = await glm_client._generate_response(analysis_prompt)
        logger.info(f"ğŸ§  GLMç¬¬{round_num}è½®åˆ†æå“åº”: {glm_response[:100]}...")
        
        # è§£æGLMå“åº”ï¼Œæå–æ–°å…³é”®è¯
        new_keywords = _parse_keywords_from_glm_response(glm_response)
        
        # è¿‡æ»¤å’Œä¼˜åŒ–å…³é”®è¯
        filtered_keywords = _filter_and_optimize_keywords(new_keywords, current_keywords, topic)
        
        logger.info(f"âœ… ç¬¬{round_num}è½®ç”Ÿæˆæ–°å…³é”®è¯: {filtered_keywords}")
        return filtered_keywords
        
    except Exception as e:
        logger.error(f"âŒ GLMç”Ÿæˆæ–°æ£€ç´¢è¯å¤±è´¥: {e}")
        return []

async def _enhance_results_with_glm_final_analysis(topic: str, original_keywords: List[str], 
                                                 all_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """ä½¿ç”¨GLM APIå¯¹æ‰€æœ‰æ£€ç´¢ç»“æœè¿›è¡Œæœ€ç»ˆåˆ†æå’Œå¢å¼º"""
    try:
        glm_client = GLMA2AClient()
        
        # æ„å»ºæœ€ç»ˆåˆ†ææç¤º
        final_analysis_prompt = f"""
        ä½œä¸ºä¸“åˆ©åˆ†æå¸ˆï¼Œè¯·å¯¹ä»¥ä¸‹ä¸“åˆ©æ£€ç´¢ç»“æœè¿›è¡Œæ·±åº¦åˆ†æå’Œå¢å¼ºï¼š

        æ£€ç´¢ä¸»é¢˜ï¼š{topic}
        åŸå§‹å…³é”®è¯ï¼š{', '.join(original_keywords)}
        æ€»æ£€ç´¢ç»“æœæ•°é‡ï¼š{len(all_results)}

        æ£€ç´¢ç»“æœæ¦‚è§ˆï¼š
        {_summarize_search_results(all_results)}

        è¯·æä¾›ï¼š
        1. æŠ€æœ¯é¢†åŸŸæ·±åº¦åˆ†æ
        2. ä¸»è¦æŠ€æœ¯ç‰¹å¾è¯†åˆ«
        3. åˆ›æ–°ç‚¹å’Œå·®å¼‚åŒ–åˆ†æ
        4. æŠ€æœ¯å‘å±•è¶‹åŠ¿é¢„æµ‹
        5. ä¸“åˆ©å¸ƒå±€å»ºè®®
        6. é£é™©è¯„ä¼°

        è¯·æä¾›ç»“æ„åŒ–çš„åˆ†æç»“æœï¼ŒåŒ…å«å…·ä½“çš„æŠ€æœ¯æ´å¯Ÿå’Œå»ºè®®ã€‚
        """
        
        final_glm_analysis = await glm_client._generate_response(final_analysis_prompt)
        logger.info(f"ğŸ§  GLMæœ€ç»ˆåˆ†æå®Œæˆï¼Œåˆ†æé•¿åº¦: {len(final_glm_analysis)}")
        
        # å°†GLMåˆ†æç»“æœæ•´åˆåˆ°æ¯ä¸ªæ£€ç´¢ç»“æœä¸­
        enhanced_results = []
        for i, result in enumerate(all_results):
            enhanced_result = result.copy()
            enhanced_result.update({
                "glm_final_analysis": final_glm_analysis[:300] + "..." if len(final_glm_analysis) > 300 else final_glm_analysis,
                "analysis_round": "final",
                "enhanced_by_glm": True,
                "technical_insights": f"GLMæ·±åº¦åˆ†æ: {final_glm_analysis[:100]}...",
                "similarity_analysis": {
                    "concept_overlap": "GLMåˆ†æï¼šæ¦‚å¿µé‡å åº¦æ·±åº¦è¯„ä¼°",
                    "technical_similarity": "GLMåˆ†æï¼šæŠ€æœ¯ç›¸ä¼¼æ€§ç»¼åˆåˆ†æ",
                    "implementation_differences": "GLMåˆ†æï¼šå®ç°å·®å¼‚æ·±åº¦è¯†åˆ«",
                    "innovation_potential": "GLMåˆ†æï¼šåˆ›æ–°æ½œåŠ›è¯„ä¼°"
                }
            })
            enhanced_results.append(enhanced_result)
        
        return enhanced_results
        
    except Exception as e:
        logger.error(f"âŒ GLMæœ€ç»ˆåˆ†æå¤±è´¥: {e}")
        return all_results

def _summarize_search_results(results: List[Dict[str, Any]]) -> str:
    """æ±‡æ€»æ£€ç´¢ç»“æœï¼Œç”¨äºGLMåˆ†æ"""
    if not results:
        return "æ— æ£€ç´¢ç»“æœ"
    
    summary = []
    for i, result in enumerate(results[:5]):  # åªå–å‰5ä¸ªç»“æœè¿›è¡Œæ‘˜è¦
        title = result.get("title", "æ— æ ‡é¢˜")
        abstract = result.get("abstract", "æ— æ‘˜è¦")
        summary.append(f"ç»“æœ{i+1}: {title} - {abstract[:100]}...")
    
    return "\n".join(summary)

def _parse_keywords_from_glm_response(glm_response: str) -> List[str]:
    """ä»GLMå“åº”ä¸­è§£æå…³é”®è¯"""
    try:
        # æ¸…ç†å“åº”æ–‡æœ¬
        cleaned_response = glm_response.strip()
        
        # å°è¯•å¤šç§åˆ†éš”ç¬¦
        separators = [',', 'ï¼Œ', ';', 'ï¼›', '\n', 'ã€']
        keywords = []
        
        for sep in separators:
            if sep in cleaned_response:
                keywords = [kw.strip() for kw in cleaned_response.split(sep) if kw.strip()]
                break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ†éš”ç¬¦ï¼Œå°è¯•æŒ‰ç©ºæ ¼åˆ†å‰²
        if not keywords:
            keywords = [kw.strip() for kw in cleaned_response.split() if kw.strip()]
        
        # è¿‡æ»¤å’Œæ¸…ç†å…³é”®è¯
        filtered_keywords = []
        for kw in keywords:
            # ç§»é™¤å¸¸è§çš„æ— å…³è¯æ±‡
            if len(kw) > 1 and kw not in ['çš„', 'å’Œ', 'ä¸', 'æˆ–', 'ç­‰', 'ç­‰', 'æŠ€æœ¯', 'ç³»ç»Ÿ', 'æ–¹æ³•']:
                filtered_keywords.append(kw)
        
        return filtered_keywords[:8]  # é™åˆ¶å…³é”®è¯æ•°é‡
        
    except Exception as e:
        logger.warning(f"âš ï¸ è§£æGLMå…³é”®è¯å¤±è´¥: {e}")
        return []

def _filter_and_optimize_keywords(new_keywords: List[str], current_keywords: List[str], 
                                topic: str) -> List[str]:
    """è¿‡æ»¤å’Œä¼˜åŒ–å…³é”®è¯"""
    try:
        optimized = []
        
        for kw in new_keywords:
            # é¿å…é‡å¤
            if kw not in current_keywords and kw not in optimized:
                # ç¡®ä¿å…³é”®è¯ä¸ä¸»é¢˜ç›¸å…³
                if any(word in kw.lower() for word in topic.lower().split()) or \
                   any(word in kw.lower() for word in current_keywords):
                    optimized.append(kw)
                else:
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æŠ€æœ¯ç›¸å…³è¯æ±‡
                    tech_terms = ['æŠ€æœ¯', 'ç³»ç»Ÿ', 'æ–¹æ³•', 'è®¾å¤‡', 'ç®—æ³•', 'æ¨¡å‹', 'ç½‘ç»œ', 'æ•°æ®', 'æ™ºèƒ½', 'è‡ªåŠ¨']
                    if any(term in kw for term in tech_terms):
                        optimized.append(kw)
        
        # å¦‚æœæ²¡æœ‰ä¼˜åŒ–ç»“æœï¼Œè¿”å›åŸå§‹æ–°å…³é”®è¯
        if not optimized:
            return new_keywords[:5]
        
        return optimized[:5]  # é™åˆ¶æ•°é‡
        
    except Exception as e:
        logger.warning(f"âš ï¸ å…³é”®è¯ä¼˜åŒ–å¤±è´¥: {e}")
        return new_keywords[:5]

async def _search_with_duckduckgo_api(topic: str, keywords: List[str], max_results: int) -> List[Dict[str, Any]]:
    """ä½¿ç”¨DuckDuckGo APIè¿›è¡Œä¸“åˆ©æ£€ç´¢"""
    try:
        import requests
        from urllib.parse import quote_plus
        
        # æ„å»ºæœç´¢æŸ¥è¯¢
        search_query = f"patent prior art {topic} {' '.join(keywords)}"
        encoded_query = quote_plus(search_query)
        
        # DuckDuckGo API URL
        url = f"https://api.duckduckgo.com/?q={encoded_query}&format=json&no_html=1&skip_disambig=1"
        
        logger.info(f"ğŸŒ è°ƒç”¨DuckDuckGo API: {url}")
        
        # å‘é€è¯·æ±‚
        response = requests.get(url, timeout=15)
        response.raise_for_status()
        
        data = response.json()
        results = []
        
        # å¤„ç†æ‘˜è¦ç»“æœ
        if data.get('Abstract'):
            results.append({
                "patent_id": "DDG_001",
                "title": data.get('AbstractSource', f'{topic}ç›¸å…³æŠ€æœ¯'),
                "abstract": data.get('Abstract', f"æ£€ç´¢ç»“æœ: {topic}"),
                "filing_date": "N/A",
                "publication_date": "N/A",
                "assignee": "Various",
                "relevance_score": 0.85,
                "similarity_analysis": {
                    "concept_overlap": "é«˜",
                    "technical_similarity": "ä¸­ç­‰",
                    "implementation_differences": "æ˜¾è‘—"
                }
            })
        
        # å¤„ç†ç›¸å…³ä¸»é¢˜
        if data.get('RelatedTopics'):
            for i, topic_info in enumerate(data['RelatedTopics'][:max_results-1]):
                if isinstance(topic_info, dict) and topic_info.get('Text'):
                    results.append({
                        "patent_id": f"DDG_{i+2:03d}",
                        "title": topic_info.get('Text', f'ç›¸å…³ä¸»é¢˜{i+2}'),
                        "abstract": f"ä¸{topic}ç›¸å…³çš„æŠ€æœ¯ä¿¡æ¯",
                        "filing_date": "N/A",
                        "publication_date": "N/A",
                        "assignee": "Various",
                        "relevance_score": 0.75 - i * 0.05,
                        "similarity_analysis": {
                            "concept_overlap": "ä¸­ç­‰",
                            "technical_similarity": "ä¸­ç­‰",
                            "implementation_differences": "ä¸­ç­‰"
                        }
                    })
        
        # å¦‚æœæ²¡æœ‰ç»“æœï¼Œæ·»åŠ é»˜è®¤ç»“æœ
        if not results:
            results.append({
                "patent_id": "DDG_DEFAULT",
                "title": f"{topic}æŠ€æœ¯æ£€ç´¢",
                "abstract": f"ä½¿ç”¨DuckDuckGoæ£€ç´¢{topic}ç›¸å…³æŠ€æœ¯ä¿¡æ¯",
                "filing_date": "N/A",
                "publication_date": "N/A",
                "assignee": "Various",
                "relevance_score": 0.7,
                "similarity_analysis": {
                    "concept_overlap": "å¾…åˆ†æ",
                    "technical_similarity": "å¾…åˆ†æ",
                    "implementation_differences": "å¾…åˆ†æ"
                }
            })
        
        logger.info(f"âœ… DuckDuckGo APIæ£€ç´¢å®Œæˆï¼Œè·å¾— {len(results)} ä¸ªç»“æœ")
        return results[:max_results]
        
    except Exception as e:
        logger.error(f"âŒ DuckDuckGo APIè°ƒç”¨å¤±è´¥: {e}")
        raise

def _get_mock_search_results(topic: str, keywords: List[str]) -> List[Dict[str, Any]]:
    """è·å–mockæ£€ç´¢ç»“æœï¼ˆfallbackï¼‰"""
    logger.info("ğŸ“ ä½¿ç”¨mockæ•°æ®è¿›è¡Œç°æœ‰æŠ€æœ¯æ£€ç´¢")
    return [
        {
            "patent_id": "US1234567",
            "title": "æ™ºèƒ½å‚æ•°æ¨æ–­ç³»ç»Ÿ",
            "abstract": "åŸºäºä¸Šä¸‹æ–‡å’Œç”¨æˆ·æ„å›¾è‡ªåŠ¨æ¨æ–­å·¥å…·è°ƒç”¨å‚æ•°çš„ç³»ç»Ÿ",
            "filing_date": "2022-01-15",
            "publication_date": "2023-07-20",
            "assignee": "ç§‘æŠ€å…¬å¸",
            "relevance_score": 0.85,
            "similarity_analysis": {
                "concept_overlap": "é«˜",
                "technical_similarity": "ä¸­ç­‰",
                "implementation_differences": "æ˜¾è‘—"
            }
        }
    ]

async def analyze_search_results(search_results: List[Dict[str, Any]], topic: str) -> Dict[str, Any]:
    """Analyze search results (mock implementation)"""
    logger.info(f"ğŸ“Š Analyzing search results for: {topic}")
    
    # ä¿®å¤ï¼šç¡®ä¿search_resultsæ˜¯åˆ—è¡¨ç±»å‹
    if not isinstance(search_results, list):
        logger.warning(f"âš ï¸ search_resultsä¸æ˜¯åˆ—è¡¨ç±»å‹: {type(search_results)}ï¼Œè½¬æ¢ä¸ºç©ºåˆ—è¡¨")
        search_results = []
    
    # å®‰å…¨åœ°åˆ†ææœç´¢ç»“æœ
    try:
        high_relevance_count = len([r for r in search_results if isinstance(r, dict) and r.get("relevance_score", 0) > 0.8])
        medium_relevance_count = len([r for r in search_results if isinstance(r, dict) and 0.5 <= r.get("relevance_score", 0) <= 0.8])
        low_relevance_count = len([r for r in search_results if isinstance(r, dict) and r.get("relevance_score", 0) < 0.5])
    except Exception as e:
        logger.warning(f"âš ï¸ åˆ†æç›¸å…³æ€§åˆ†æ•°æ—¶å‡ºé”™: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
        high_relevance_count = 0
        medium_relevance_count = 0
        low_relevance_count = len(search_results)
    
    return {
        "total_patents_found": len(search_results),
        "high_relevance_count": high_relevance_count,
        "medium_relevance_count": medium_relevance_count,
        "low_relevance_count": low_relevance_count,
        "technology_trends": [
            "Increasing focus on intelligent parameter inference",
            "Growing adoption of context-aware systems",
            "Rising interest in adaptive tool calling"
        ],
        "competitive_landscape": {
            "major_players": ["Tech Corp Inc", "Innovation Labs LLC", "Advanced Systems Corp"],
            "market_concentration": "Medium",
            "entry_barriers": "High"
        },
        "technical_gaps": [
            "Limited focus on layered reasoning approaches",
            "Insufficient attention to user intent modeling",
            "Lack of comprehensive multi-parameter optimization"
        ]
    }

async def assess_novelty(search_results: List[Dict[str, Any]], analysis: Dict[str, Any]) -> Dict[str, Any]:
    """Assess novelty and patentability (mock implementation)"""
    logger.info("ğŸ¯ Assessing novelty and patentability")
    return {
        "novelty_score": 8.0,
        "inventive_step_score": 7.5,
        "industrial_applicability": True,
        "risk_level": "Low to Medium",
        "risk_factors": {
            "prior_art_risks": {
                "probability": "Low",
                "impact": "Medium",
                "mitigation": "Focus on unique layered reasoning approach"
            },
            "obviousness_risks": {
                "probability": "Medium",
                "impact": "High",
                "mitigation": "Emphasize non-obvious technical solutions"
            }
        },
        "patentability_assessment": "Strong",
        "key_differentiators": [
            "Layered reasoning architecture",
            "Multi-parameter adaptive optimization",
            "Context-aware user intent modeling"
        ]
    }

async def generate_recommendations(search_results: List[Dict[str, Any]], analysis: Dict[str, Any], novelty_assessment: Dict[str, Any]) -> List[str]:
    """Generate recommendations (mock implementation)"""
    logger.info("ğŸ’¡ Generating recommendations")
    return [
        "Proceed with patent filing - strong novelty and inventive step identified",
        "Focus on layered reasoning architecture as key differentiator",
        "Emphasize multi-parameter optimization capabilities",
        "Consider design-around strategies for identified prior art",
        "Accelerate filing timeline to establish priority",
        "Include comprehensive technical diagrams and examples"
    ]

if __name__ == "__main__":
    print("ğŸš€ Starting Unified Patent Agent System v2.0.0...")
    print("ğŸ”§ Test mode: DEPRECATED - Now using workflow-specific test_mode")
    print("â±ï¸ Test delay: Configurable per workflow")
    print("ğŸ“¡ Single service will be available at: http://localhost:8000")
    print("ğŸ“š API docs will be available at: http://localhost:8000/docs")
    print("ğŸ¤– All agents available at:")
    print("   - Coordinator: /coordinator/* (Patent workflows only)")
    print("   - Planner: /agents/planner/*")
    print("   - Searcher: /agents/searcher/*")
    print("   - Discussion: /agents/discussion/*")
    print("   - Writer: /agents/writer/*")
    print("   - Reviewer: /agents/reviewer/*")
    print("   - Rewriter: /agents/rewriter/*")
    print("ğŸ“‹ Coordinator API endpoints (Patent workflows only):")
    print("   - POST /coordinator/workflow/start - Start patent workflow")
    print("   - GET /coordinator/workflow/{workflow_id}/status - Get patent workflow status")
    print("   - GET /coordinator/workflow/{workflow_id}/results - Get patent workflow results")
    print("   - GET /workflow/{workflow_id}/progress - Get real-time workflow progress")
    print("   - GET /workflow/{workflow_id}/stages - Get workflow stage files")
    print("   - WS /ws/workflow/{workflow_id} - Real-time workflow updates (WebSocket)")
    print("   - GET /download/workflow/{workflow_id} - Download entire workflow directory (ZIP)")
    print("   - GET /download/patent/{workflow_id} - Download final patent file only")
    print("   - POST /coordinator/workflow/{workflow_id}/restart - Restart patent workflow")
    print("   - DELETE /coordinator/workflow/{workflow_id} - Delete patent workflow")
    print("   - GET /coordinator/workflows - List all patent workflows")
    print("ğŸ”§ Test mode endpoints:")
    print("   - GET /test-mode - Check test mode status")
    print("   - POST /test-mode - Update test mode settings")
    uvicorn.run(app, host="0.0.0.0", port=8000)