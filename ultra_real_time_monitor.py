#!/usr/bin/env python3
"""
Ultra Real-Time Workflow Monitor - Log-Based Only
è¶…å®æ—¶å·¥ä½œæµç›‘æ§ç³»ç»Ÿ - ä»…åŸºäºæ—¥å¿—æ–‡ä»¶ç›‘æ§ï¼Œä¸å‘åè°ƒå™¨å‘é€ä»»ä½•ä»»åŠ¡
"""

import asyncio
import os
import sys
import time
import logging
import json
import hashlib
from datetime import datetime
from typing import Dict, Any, List, Optional, Set
from pathlib import Path
import subprocess
import threading
import glob

# Add project path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'patent_agent_demo'))

from patent_agent_demo.patent_agent_system import PatentAgentSystem

class UltraRealTimeMonitor:
    """è¶…å®æ—¶å·¥ä½œæµç›‘æ§å™¨ - ä»…åŸºäºæ—¥å¿—æ–‡ä»¶"""
    
    def __init__(self, workflow_id: str = None):
        self.workflow_id = workflow_id
        # ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œåœ¨å½“å‰é¡¹ç›®ç›®å½•ä¸‹
        self.monitor_dir = os.path.dirname(__file__)
        # ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œåœ¨å½“å‰é¡¹ç›®ç›®å½•ä¸‹åˆ›å»ºoutputæ–‡ä»¶å¤¹
        self.output_dir = os.path.join(os.path.dirname(__file__), "output")
        self.log_file = os.path.join(os.path.dirname(__file__), "ultra_monitor.log")
        self.monitoring = False
        self.start_time = None
        self.last_status = None
        self.status_history = []
        self.file_changes = []
        self.last_file_sizes = {}
        self.last_file_hashes = {}
        self.last_modified = {}
        self.monitor_thread = None
        
        # Create output directory if not exists
        os.makedirs(self.output_dir, exist_ok=True)
        
        # Setup logging with millisecond precision
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s.%(msecs)03d - %(levelname)s - %(message)s',
            datefmt='%H:%M:%S',
            handlers=[
                logging.FileHandler(self.log_file),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    async def start_monitoring(self, topic: str, description: str):
        """å¯åŠ¨è¶…å®æ—¶ç›‘æ§ - ä»…åŸºäºæ—¥å¿—æ–‡ä»¶"""
        try:
            self.logger.info("ğŸš€ å¯åŠ¨è¶…å®æ—¶å·¥ä½œæµç›‘æ§ç³»ç»Ÿ - ä»…åŸºäºæ—¥å¿—æ–‡ä»¶")
            self.logger.info("ğŸ“‹ ç›‘æ§ç­–ç•¥: ä»…é€šè¿‡æ—¥å¿—æ–‡ä»¶ç›‘æ§ï¼Œä¸å‘åè°ƒå™¨å‘é€ä»»ä½•ä»»åŠ¡")
            self.start_time = time.time()
            self.monitoring = True
            
            # Start file monitoring thread
            self._start_file_monitoring_thread()
            
            # Start real-time monitoring loop
            await self._ultra_monitor_loop()
            
        except Exception as e:
            self.logger.error(f"ç›‘æ§å¯åŠ¨å¤±è´¥: {e}")
            raise
            
    def _start_file_monitoring_thread(self):
        """å¯åŠ¨æ–‡ä»¶ç›‘æ§çº¿ç¨‹"""
        def file_monitor_loop():
            while self.monitoring:
                try:
                    self._check_all_files()
                    time.sleep(0.5)  # Check every 500ms
                except Exception as e:
                    self.logger.error(f"æ–‡ä»¶ç›‘æ§é”™è¯¯: {e}")
                    
        self.monitor_thread = threading.Thread(target=file_monitor_loop, daemon=True)
        self.monitor_thread.start()
        self.logger.info("âœ… æ–‡ä»¶ç›‘æ§çº¿ç¨‹å¯åŠ¨æˆåŠŸ")
        
    async def _ultra_monitor_loop(self):
        """è¶…å®æ—¶ç›‘æ§ä¸»å¾ªç¯ - ä»…åŸºäºæ—¥å¿—æ–‡ä»¶"""
        try:
            self.logger.info("ğŸ”„ å¼€å§‹è¶…å®æ—¶ç›‘æ§å¾ªç¯")
            
            while self.monitoring:
                try:
                    # æ£€æŸ¥æ™ºèƒ½ä½“æ—¥å¿—æ–‡ä»¶
                    await self._check_agent_logs()
                    
                    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
                    await self._check_output_files()
                    
                    # æ£€æŸ¥ç³»ç»Ÿèµ„æº
                    await self._check_system_resources()
                    
                    # æ£€æŸ¥å·¥ä½œæµçŠ¶æ€ï¼ˆåŸºäºæ—¥å¿—æ–‡ä»¶ï¼‰
                    await self._check_workflow_status_from_logs()
                    
                    # ç­‰å¾…ä¸‹ä¸€æ¬¡æ£€æŸ¥
                    await asyncio.sleep(1)  # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡
                    
                except Exception as e:
                    self.logger.error(f"ç›‘æ§å¾ªç¯é”™è¯¯: {e}")
                    await asyncio.sleep(5)  # å‡ºé”™æ—¶ç­‰å¾…5ç§’
                    
        except Exception as e:
            self.logger.error(f"ç›‘æ§å¾ªç¯å¤±è´¥: {e}")
            
    async def _check_agent_logs(self):
        """æ£€æŸ¥æ™ºèƒ½ä½“æ—¥å¿—æ–‡ä»¶"""
        try:
            logs_dir = os.path.join(self.output_dir, "logs")
            if not os.path.exists(logs_dir):
                return
                
            # Check each agent log file
            agent_logs = [
                "coordinator_agent.log",
                "planner_agent.log", 
                "searcher_agent.log",
                "discusser_agent.log",
                "writer_agent.log",
                "reviewer_agent.log",
                "rewriter_agent.log"
            ]
            
            for log_file in agent_logs:
                log_path = os.path.join(logs_dir, log_file)
                if os.path.exists(log_path):
                    try:
                        file_stat = os.stat(log_path)
                        file_size = file_stat.st_size
                        file_mtime = file_stat.st_mtime
                        
                        # Check if log file changed
                        file_key = log_path
                        if file_key not in self.last_modified or self.last_modified[file_key] != file_mtime:
                            self.last_modified[file_key] = file_mtime
                            
                            # Read last few lines of the log
                            try:
                                with open(log_path, 'r', encoding='utf-8') as f:
                                    lines = f.readlines()
                                    if lines:
                                        last_line = lines[-1].strip()
                                        # Only log important events (not heartbeat messages)
                                        if any(keyword in last_line for keyword in ["âœ…", "âŒ", "âš ï¸", "ğŸ¯", "ğŸš€", "ğŸ“¤", "ğŸ“‹"]):
                                            agent_name = log_file.replace("_agent.log", "")
                                            self.logger.info(f"ğŸ¤– {agent_name}: {last_line}")
                                            
                            except Exception as e:
                                pass  # Skip unreadable log files
                                
                    except Exception as e:
                        pass  # Skip files with errors
                        
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥æ™ºèƒ½ä½“æ—¥å¿—å¤±è´¥: {e}")
            
    async def _check_workflow_status_from_logs(self):
        """ä»æ—¥å¿—æ–‡ä»¶æ£€æŸ¥å·¥ä½œæµçŠ¶æ€"""
        try:
            logs_dir = os.path.join(self.output_dir, "logs")
            if not os.path.exists(logs_dir):
                return
                
            # æ£€æŸ¥åè°ƒå™¨æ—¥å¿—ä¸­çš„å·¥ä½œæµçŠ¶æ€
            coordinator_log = os.path.join(logs_dir, "coordinator_agent.log")
            if os.path.exists(coordinator_log):
                try:
                    with open(coordinator_log, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                        # æ£€æŸ¥æœ€å100è¡Œ
                        recent_lines = lines[-100:] if len(lines) > 100 else lines
                        
                        # åˆ†æå·¥ä½œæµçŠ¶æ€
                        workflow_status = self._analyze_workflow_status_from_logs(recent_lines)
                        
                        if workflow_status != self.last_status:
                            self.last_status = workflow_status
                            self.status_history.append(workflow_status)
                            self.logger.info(f"ğŸ”¥ å·¥ä½œæµçŠ¶æ€æ›´æ–°: {workflow_status}")
                            
                except Exception as e:
                    pass  # Skip unreadable log files
                    
        except Exception as e:
            self.logger.error(f"ä»æ—¥å¿—æ£€æŸ¥å·¥ä½œæµçŠ¶æ€å¤±è´¥: {e}")
            
    def _analyze_workflow_status_from_logs(self, log_lines: List[str]) -> Dict[str, Any]:
        """ä»æ—¥å¿—è¡Œåˆ†æå·¥ä½œæµçŠ¶æ€"""
        try:
            status = {
                "timestamp": datetime.now().strftime("%H:%M:%S.%f")[:-3],
                "overall_status": "running",
                "current_stage": 0,
                "total_stages": 7,  # é»˜è®¤7ä¸ªé˜¶æ®µ
                "progress": "0/7"
            }
            
            # åˆ†ææ—¥å¿—å†…å®¹
            for line in log_lines:
                line = line.strip()
                
                # æ£€æŸ¥é˜¶æ®µå®Œæˆæƒ…å†µ
                if "âœ… é˜¶æ®µå®Œæˆ" in line or "ğŸ¯ é˜¶æ®µ" in line:
                    if "planner" in line.lower():
                        status["current_stage"] = max(status["current_stage"], 1)
                    elif "searcher" in line.lower():
                        status["current_stage"] = max(status["current_stage"], 2)
                    elif "discusser" in line.lower():
                        status["current_stage"] = max(status["current_stage"], 3)
                    elif "writer" in line.lower():
                        status["current_stage"] = max(status["current_stage"], 4)
                    elif "reviewer" in line.lower():
                        status["current_stage"] = max(status["current_stage"], 5)
                    elif "rewriter" in line.lower():
                        status["current_stage"] = max(status["current_stage"], 6)
                        
                # æ£€æŸ¥å·¥ä½œæµå®Œæˆ
                if "ğŸ‰ å·¥ä½œæµå®Œæˆ" in line or "âœ… å·¥ä½œæµå®Œæˆ" in line:
                    status["overall_status"] = "completed"
                    status["current_stage"] = 7
                    
                # æ£€æŸ¥å·¥ä½œæµå¤±è´¥
                if "âŒ å·¥ä½œæµå¤±è´¥" in line or "error" in line.lower():
                    status["overall_status"] = "failed"
                    
            # æ›´æ–°è¿›åº¦
            status["progress"] = f"{status['current_stage']}/{status['total_stages']}"
            
            return status
            
        except Exception as e:
            return {
                "timestamp": datetime.now().strftime("%H:%M:%S.%f")[:-3],
                "overall_status": "unknown",
                "current_stage": 0,
                "total_stages": 7,
                "progress": "0/7",
                "error": str(e)
            }
            
    async def _check_system_resources(self):
        """æ£€æŸ¥ç³»ç»Ÿèµ„æº"""
        try:
            # Get basic system info
            import psutil
            cpu_percent = psutil.cpu_percent(interval=0.1)
            memory = psutil.virtual_memory()
            
            # Get process information
            current_process = psutil.Process()
            process_cpu = current_process.cpu_percent()
            process_memory = current_process.memory_info().rss / 1024 / 1024  # MB
            
            resource_info = {
                "timestamp": datetime.now().strftime("%H:%M:%S.%f")[:-3],
                "system_cpu": cpu_percent,
                "system_memory_percent": memory.percent,
                "process_cpu": process_cpu,
                "process_memory_mb": round(process_memory, 2)
            }
            
            # Log resource usage (less frequent)
            if int(time.time()) % 30 == 0:  # æ¯30ç§’è®°å½•ä¸€æ¬¡
                self.logger.info(f"ğŸ’» èµ„æºä½¿ç”¨: CPU {cpu_percent}% | å†…å­˜ {memory.percent}% | è¿›ç¨‹å†…å­˜ {process_memory:.1f}MB")
            
        except ImportError:
            # Fallback without psutil
            try:
                # Get basic memory info from /proc
                with open('/proc/meminfo', 'r') as f:
                    meminfo = f.read()
                    
                # Parse memory info
                total_mem = 0
                free_mem = 0
                for line in meminfo.split('\n'):
                    if line.startswith('MemTotal:'):
                        total_mem = int(line.split()[1])
                    elif line.startswith('MemAvailable:'):
                        free_mem = int(line.split()[1])
                        
                if total_mem > 0:
                    memory_percent = ((total_mem - free_mem) / total_mem) * 100
                    if int(time.time()) % 30 == 0:  # æ¯30ç§’è®°å½•ä¸€æ¬¡
                        self.logger.info(f"ğŸ’» å†…å­˜ä½¿ç”¨: {memory_percent:.1f}%")
                    
            except Exception as e:
                if int(time.time()) % 30 == 0:  # æ¯30ç§’è®°å½•ä¸€æ¬¡
                    self.logger.info(f"ğŸ’» ç³»ç»Ÿèµ„æºç›‘æ§: åŸºç¡€æ¨¡å¼")
                
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥ç³»ç»Ÿèµ„æºå¤±è´¥: {e}")
            
    async def _check_output_files(self):
        """æ£€æŸ¥è¾“å‡ºæ–‡ä»¶"""
        try:
            # Check for new output files
            output_files = []
            if os.path.exists(self.output_dir):
                for file in os.listdir(self.output_dir):
                    if file.endswith('.md') or file.endswith('.txt') or file.endswith('.json'):
                        file_path = os.path.join(self.output_dir, file)
                        output_files.append(file_path)
                        
            # Check for changes in existing files
            for file_path in output_files:
                try:
                    file_stat = os.stat(file_path)
                    file_size = file_stat.st_size
                    file_mtime = file_stat.st_mtime
                    
                    # Check if file changed
                    file_key = file_path
                    if file_key not in self.last_modified or self.last_modified[file_key] != file_mtime:
                        self.last_modified[file_key] = file_mtime
                        
                        # Get file size change
                        old_size = self.last_file_sizes.get(file_key, 0)
                        size_change = file_size - old_size
                        self.last_file_sizes[file_key] = file_size
                        
                        # Log file change
                        file_name = os.path.basename(file_path)
                        self.logger.info(f"ğŸ“„ æ–‡ä»¶æ›´æ–°: {file_name} | å¤§å°å˜åŒ–: {size_change:+d} bytes | æ€»å¤§å°: {file_size} bytes")
                        
                        # Record file change
                        self.file_changes.append({
                            "timestamp": datetime.now().strftime("%H:%M:%S.%f")[:-3],
                            "file": file_name,
                            "size_change": size_change,
                            "total_size": file_size
                        })
                        
                except Exception as e:
                    pass  # Skip files with errors
                    
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥è¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}")
            
    def _check_all_files(self):
        """æ£€æŸ¥æ‰€æœ‰ç›¸å…³æ–‡ä»¶"""
        try:
            # Check log files
            logs_dir = os.path.join(self.output_dir, "logs")
            if os.path.exists(logs_dir):
                for log_file in os.listdir(logs_dir):
                    if log_file.endswith('.log'):
                        log_path = os.path.join(logs_dir, log_file)
                        try:
                            file_stat = os.stat(log_path)
                            file_size = file_stat.st_size
                            file_mtime = file_stat.st_mtime
                            
                            # Check if log file changed
                            file_key = log_path
                            if file_key not in self.last_modified or self.last_modified[file_key] != file_mtime:
                                self.last_modified[file_key] = file_mtime
                                
                                # Read last line of the log
                                try:
                                    with open(log_path, 'r', encoding='utf-8') as f:
                                        lines = f.readlines()
                                        if lines:
                                            last_line = lines[-1].strip()
                                            # Only log important events
                                            if any(keyword in last_line for keyword in ["âœ…", "âŒ", "âš ï¸", "ğŸ¯", "ğŸš€", "ğŸ“¤", "ğŸ“‹"]):
                                                agent_name = log_file.replace("_agent.log", "")
                                                self.logger.info(f"ğŸ¤– {agent_name}: {last_line}")
                                                
                                except Exception as e:
                                    pass  # Skip unreadable log files
                                    
                        except Exception as e:
                            pass  # Skip files with errors
                            
        except Exception as e:
            pass  # Silent error handling for file monitoring
            
    async def _save_status_update(self, status: Dict[str, Any]):
        """ä¿å­˜çŠ¶æ€æ›´æ–°åˆ°æ–‡ä»¶"""
        try:
            status_file = os.path.join(self.output_dir, "workflow_status.json")
            
            # Load existing status history
            status_history = []
            if os.path.exists(status_file):
                try:
                    with open(status_file, 'r', encoding='utf-8') as f:
                        status_history = json.load(f)
                except Exception as e:
                    self.logger.warning(f"è¯»å–çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")
                    
            # Add new status
            status_history.append(status)
            
            # Keep only last 100 status updates
            if len(status_history) > 100:
                status_history = status_history[-100:]
                
            # Save updated status history
            with open(status_file, 'w', encoding='utf-8') as f:
                json.dump(status_history, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            self.logger.error(f"ä¿å­˜çŠ¶æ€æ›´æ–°å¤±è´¥: {e}")
            
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.logger.info("ğŸ›‘ åœæ­¢è¶…å®æ—¶ç›‘æ§")
        self.monitoring = False
        
        if self.monitor_thread and self.monitor_thread.is_alive():
            self.monitor_thread.join(timeout=5)
            
        # Save final status
        if self.status_history:
            final_status = {
                "timestamp": datetime.now().strftime("%H:%M:%S.%f")[:-3],
                "monitoring_duration": time.time() - self.start_time if self.start_time else 0,
                "total_status_updates": len(self.status_history),
                "total_file_changes": len(self.file_changes),
                "final_status": self.status_history[-1] if self.status_history else None
            }
            
            try:
                final_status_file = os.path.join(self.output_dir, "monitoring_summary.json")
                with open(final_status_file, 'w', encoding='utf-8') as f:
                    json.dump(final_status, f, ensure_ascii=False, indent=2)
                self.logger.info("âœ… ç›‘æ§æ‘˜è¦å·²ä¿å­˜")
            except Exception as e:
                self.logger.error(f"ä¿å­˜ç›‘æ§æ‘˜è¦å¤±è´¥: {e}")
                
    def get_monitoring_summary(self) -> Dict[str, Any]:
        """è·å–ç›‘æ§æ‘˜è¦"""
        return {
            "monitoring_duration": time.time() - self.start_time if self.start_time else 0,
            "total_status_updates": len(self.status_history),
            "total_file_changes": len(self.file_changes),
            "current_status": self.last_status,
            "file_changes": self.file_changes[-10:] if self.file_changes else [],  # Last 10 changes
            "status_history": self.status_history[-10:] if self.status_history else []  # Last 10 status updates
        }

async def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºç›‘æ§å™¨
        monitor = UltraRealTimeMonitor()
        
        # è®¾ç½®ä¸»é¢˜å’Œæè¿°
        topic = "æ™ºèƒ½å¤šå‚æ•°å·¥å…·è°ƒç”¨ç³»ç»Ÿ"
        description = "ä¸€ç§åŸºäºæ™ºèƒ½åˆ†å±‚æ¨ç†çš„å¤šå‚æ•°å·¥å…·è‡ªé€‚åº”è°ƒç”¨ç³»ç»Ÿï¼Œé€šè¿‡åˆ†å±‚æ¨ç†å’Œè‡ªé€‚åº”æœºåˆ¶æé«˜è°ƒç”¨å‡†ç¡®æ€§"
        
        # å¯åŠ¨ç›‘æ§
        await monitor.start_monitoring(topic, description)
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­ç›‘æ§")
    except Exception as e:
        print(f"âŒ ç›‘æ§å¤±è´¥: {e}")
    finally:
        if 'monitor' in locals():
            monitor.stop_monitoring()

if __name__ == "__main__":
    asyncio.run(main())