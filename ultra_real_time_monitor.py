#!/usr/bin/env python3
"""
Ultra Real-Time Workflow Monitor
è¶…å®æ—¶å·¥ä½œæµç›‘æ§ç³»ç»Ÿ - æ— éœ€å¤–éƒ¨ä¾èµ–ï¼Œå®æ—¶ç›‘æ§å·¥ä½œæµè¿›åº¦
"""

import asyncio
import os
import sys
import time
import logging
import json
import hashlib
from datetime import datetime
from typing import Dict, Any, List, Optional, Set
from pathlib import Path
import subprocess
import threading
import glob

# Add project path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'patent_agent_demo'))

from patent_agent_demo.patent_agent_system import PatentAgentSystem

class UltraRealTimeMonitor:
    """è¶…å®æ—¶å·¥ä½œæµç›‘æ§å™¨"""
    
    def __init__(self, workflow_id: str = None):
        self.workflow_id = workflow_id
        self.monitor_dir = "/workspace"
        self.output_dir = "/workspace/output"
        self.log_file = "/workspace/ultra_monitor.log"
        self.monitoring = False
        self.start_time = None
        self.last_status = None
        self.status_history = []
        self.file_changes = []
        self.system = None
        self.last_file_sizes = {}
        self.last_file_hashes = {}
        self.last_modified = {}
        self.monitor_thread = None
        
        # Create output directory if not exists
        os.makedirs(self.output_dir, exist_ok=True)
        
        # Setup logging with millisecond precision
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s.%(msecs)03d - %(levelname)s - %(message)s',
            datefmt='%H:%M:%S',
            handlers=[
                logging.FileHandler(self.log_file),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    async def start_monitoring(self, topic: str, description: str):
        """å¯åŠ¨è¶…å®æ—¶ç›‘æ§"""
        try:
            self.logger.info("ğŸš€ å¯åŠ¨è¶…å®æ—¶å·¥ä½œæµç›‘æ§ç³»ç»Ÿ")
            self.start_time = time.time()
            self.monitoring = True
            
            # Start workflow
            await self._start_workflow(topic, description)
            
            # Start file monitoring thread
            self._start_file_monitoring_thread()
            
            # Start real-time monitoring loop
            await self._ultra_monitor_loop()
            
        except Exception as e:
            self.logger.error(f"ç›‘æ§å¯åŠ¨å¤±è´¥: {e}")
            raise
            
    def _start_file_monitoring_thread(self):
        """å¯åŠ¨æ–‡ä»¶ç›‘æ§çº¿ç¨‹"""
        def file_monitor_loop():
            while self.monitoring:
                try:
                    self._check_all_files()
                    time.sleep(0.5)  # Check every 500ms
                except Exception as e:
                    self.logger.error(f"æ–‡ä»¶ç›‘æ§é”™è¯¯: {e}")
                    
        self.monitor_thread = threading.Thread(target=file_monitor_loop, daemon=True)
        self.monitor_thread.start()
        self.logger.info("âœ… æ–‡ä»¶ç›‘æ§çº¿ç¨‹å¯åŠ¨æˆåŠŸ")
        
    async def _start_workflow(self, topic: str, description: str):
        """å¯åŠ¨å·¥ä½œæµ"""
        try:
            self.logger.info("ğŸ”§ å¯åŠ¨ä¸“åˆ©æ’°å†™å·¥ä½œæµ")
            self.logger.info(f"ä¸»é¢˜: {topic}")
            self.logger.info(f"æè¿°: {description}")
            
            # Initialize system
            self.system = PatentAgentSystem(test_mode=False)
            await self.system.start()
            
            # Start workflow
            self.workflow_id = await self.system.execute_workflow(
                topic=topic,
                description=description,
                workflow_type="enhanced"
            )
            
            self.logger.info(f"âœ… å·¥ä½œæµå¯åŠ¨æˆåŠŸ - ID: {self.workflow_id}")
            
        except Exception as e:
            self.logger.error(f"å·¥ä½œæµå¯åŠ¨å¤±è´¥: {e}")
            raise
            
    async def _ultra_monitor_loop(self):
        """è¶…å®æ—¶ç›‘æ§å¾ªç¯"""
        try:
            self.logger.info("ğŸ“Š å¼€å§‹è¶…å®æ—¶ç›‘æ§å¾ªç¯")
            
            while self.monitoring:
                current_time = time.time()
                
                # Monitor workflow status (every 1 second)
                if current_time - getattr(self, '_last_status_check', 0) >= 1:
                    await self._check_workflow_status()
                    self._last_status_check = current_time
                
                # Monitor system resources (every 3 seconds)
                if current_time - getattr(self, '_last_resource_check', 0) >= 3:
                    await self._check_system_resources()
                    self._last_resource_check = current_time
                
                # Monitor output files (every 0.5 seconds)
                if current_time - getattr(self, '_last_output_check', 0) >= 0.5:
                    await self._check_output_files()
                    self._last_output_check = current_time
                
                # Check for workflow completion
                if self._is_workflow_completed():
                    self.logger.info("âœ… å·¥ä½œæµå®Œæˆ")
                    break
                
                # Ultra short sleep for maximum responsiveness
                await asyncio.sleep(0.2)  # 200ms intervals
                
        except Exception as e:
            self.logger.error(f"ç›‘æ§å¾ªç¯é”™è¯¯: {e}")
            
    async def _check_workflow_status(self):
        """æ£€æŸ¥å·¥ä½œæµçŠ¶æ€"""
        try:
            if not self.system or not self.workflow_id:
                return
                
            status_result = await self.system.get_workflow_status(self.workflow_id)
            
            if status_result.get("success"):
                workflow_data = status_result.get("workflow", {})
                
                # Extract status information
                if hasattr(workflow_data, 'overall_status'):
                    overall_status = workflow_data.overall_status
                    current_stage = getattr(workflow_data, 'current_stage', 0)
                    total_stages = len(getattr(workflow_data, 'stages', []))
                elif isinstance(workflow_data, dict):
                    overall_status = workflow_data.get("overall_status", "unknown")
                    current_stage = workflow_data.get("current_stage", 0)
                    total_stages = len(workflow_data.get("stages", []))
                else:
                    overall_status = "unknown"
                    current_stage = 0
                    total_stages = 0
                
                # Check if status changed
                new_status = {
                    "timestamp": datetime.now().strftime("%H:%M:%S.%f")[:-3],
                    "overall_status": overall_status,
                    "current_stage": current_stage,
                    "total_stages": total_stages,
                    "progress": f"{current_stage}/{total_stages}" if total_stages > 0 else "0/0"
                }
                
                if new_status != self.last_status:
                    self.last_status = new_status
                    self.status_history.append(new_status)
                    
                    # Log status change with high visibility
                    self.logger.info(f"ğŸ”¥ çŠ¶æ€æ›´æ–°: {overall_status} | é˜¶æ®µ: {current_stage}/{total_stages} | æ—¶é—´: {new_status['timestamp']}")
                    
                    # Save status to file
                    await self._save_status_update(new_status)
                    
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥å·¥ä½œæµçŠ¶æ€å¤±è´¥: {e}")
            
    async def _check_system_resources(self):
        """æ£€æŸ¥ç³»ç»Ÿèµ„æº"""
        try:
            # Get basic system info
            import psutil
            cpu_percent = psutil.cpu_percent(interval=0.1)
            memory = psutil.virtual_memory()
            
            # Get process information
            current_process = psutil.Process()
            process_cpu = current_process.cpu_percent()
            process_memory = current_process.memory_info().rss / 1024 / 1024  # MB
            
            resource_info = {
                "timestamp": datetime.now().strftime("%H:%M:%S.%f")[:-3],
                "system_cpu": cpu_percent,
                "system_memory_percent": memory.percent,
                "process_cpu": process_cpu,
                "process_memory_mb": round(process_memory, 2)
            }
            
            # Log resource usage
            self.logger.info(f"ğŸ’» èµ„æºä½¿ç”¨: CPU {cpu_percent}% | å†…å­˜ {memory.percent}% | è¿›ç¨‹å†…å­˜ {process_memory:.1f}MB")
            
        except ImportError:
            # Fallback without psutil
            try:
                # Get basic memory info from /proc
                with open('/proc/meminfo', 'r') as f:
                    meminfo = f.read()
                    
                # Parse memory info
                total_mem = 0
                free_mem = 0
                for line in meminfo.split('\n'):
                    if line.startswith('MemTotal:'):
                        total_mem = int(line.split()[1])
                    elif line.startswith('MemAvailable:'):
                        free_mem = int(line.split()[1])
                        
                if total_mem > 0:
                    memory_percent = ((total_mem - free_mem) / total_mem) * 100
                    self.logger.info(f"ğŸ’» å†…å­˜ä½¿ç”¨: {memory_percent:.1f}%")
                    
            except Exception as e:
                self.logger.info(f"ğŸ’» ç³»ç»Ÿèµ„æºç›‘æ§: åŸºç¡€æ¨¡å¼")
                
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥ç³»ç»Ÿèµ„æºå¤±è´¥: {e}")
            
    async def _check_output_files(self):
        """æ£€æŸ¥è¾“å‡ºæ–‡ä»¶"""
        try:
            # Check for new patent files
            patent_files = list(Path(self.monitor_dir).glob("enhanced_patent_*.md"))
            
            for file_path in patent_files:
                file_stat = file_path.stat()
                file_size = file_stat.st_size
                file_mtime = file_stat.st_mtime
                
                # Check if this is a new or updated file
                file_key = str(file_path)
                if file_key not in self.last_modified or self.last_modified[file_key] != file_mtime:
                    self.last_modified[file_key] = file_mtime
                    
                    # Read file content
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                            
                        output_info = {
                            "timestamp": datetime.now().strftime("%H:%M:%S.%f")[:-3],
                            "file": file_path.name,
                            "size": file_size,
                            "content_length": len(content),
                            "content_preview": content[:200] + "..." if len(content) > 200 else content
                        }
                        
                        self.logger.info(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶æ›´æ–°: {file_path.name} ({file_size} bytes) - {output_info['timestamp']}")
                        
                        # Save output info
                        await self._save_output_update(output_info)
                        
                    except Exception as e:
                        self.logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                        
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥è¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}")
            
    def _check_all_files(self):
        """æ£€æŸ¥æ‰€æœ‰ç›¸å…³æ–‡ä»¶çš„å˜åŒ–"""
        try:
            # Check for various file types
            file_patterns = [
                "*.md", "*.log", "*.json", "*.txt", "*.py"
            ]
            
            for pattern in file_patterns:
                files = glob.glob(os.path.join(self.monitor_dir, pattern))
                
                for file_path in files:
                    try:
                        if os.path.exists(file_path):
                            file_size = os.path.getsize(file_path)
                            file_mtime = os.path.getmtime(file_path)
                            
                            # Check if file changed
                            file_key = file_path
                            if file_key not in self.last_file_sizes or self.last_file_sizes[file_key] != file_size:
                                self.last_file_sizes[file_key] = file_size
                                
                                # Calculate file hash for content change detection
                                try:
                                    with open(file_path, 'rb') as f:
                                        file_hash = hashlib.md5(f.read()).hexdigest()[:8]
                                        
                                    if file_key not in self.last_file_hashes or self.last_file_hashes[file_key] != file_hash:
                                        self.last_file_hashes[file_key] = file_hash
                                        
                                        # Log significant changes
                                        if file_size > 100:  # Only log files larger than 100 bytes
                                            rel_path = os.path.relpath(file_path, self.monitor_dir)
                                            current_time = datetime.now().strftime("%H:%M:%S.%f")[:-3]
                                            
                                            change_info = {
                                                "timestamp": current_time,
                                                "file": rel_path,
                                                "size": file_size,
                                                "hash": file_hash
                                            }
                                            
                                            self.file_changes.append(change_info)
                                            
                                            # Log important file changes
                                            if "enhanced_patent" in rel_path or "workflow" in rel_path:
                                                self.logger.info(f"ğŸ“ é‡è¦æ–‡ä»¶å˜åŒ–: {rel_path} ({file_size} bytes) - {current_time}")
                                                
                                except Exception as e:
                                    pass  # Skip files that can't be read
                                    
                    except Exception as e:
                        pass  # Skip files with errors
                        
        except Exception as e:
            pass  # Don't let file monitoring errors stop the main loop
            
    def _is_workflow_completed(self) -> bool:
        """æ£€æŸ¥å·¥ä½œæµæ˜¯å¦å®Œæˆ"""
        if not self.last_status:
            return False
            
        return self.last_status["overall_status"] in ["completed", "finished", "success"]
        
    async def _save_status_update(self, status: Dict[str, Any]):
        """ä¿å­˜çŠ¶æ€æ›´æ–°"""
        try:
            status_file = os.path.join(self.output_dir, f"ultra_status_{self.workflow_id}.json")
            
            # Load existing status or create new
            if os.path.exists(status_file):
                with open(status_file, 'r', encoding='utf-8') as f:
                    status_data = json.load(f)
            else:
                status_data = {"workflow_id": self.workflow_id, "status_history": []}
                
            # Add new status
            status_data["status_history"].append(status)
            status_data["last_update"] = status["timestamp"]
            
            # Save to file
            with open(status_file, 'w', encoding='utf-8') as f:
                json.dump(status_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            self.logger.error(f"ä¿å­˜çŠ¶æ€æ›´æ–°å¤±è´¥: {e}")
            
    async def _save_output_update(self, output_info: Dict[str, Any]):
        """ä¿å­˜è¾“å‡ºæ›´æ–°"""
        try:
            output_file = os.path.join(self.output_dir, f"ultra_output_{self.workflow_id}.json")
            
            # Load existing output or create new
            if os.path.exists(output_file):
                with open(output_file, 'r', encoding='utf-8') as f:
                    output_data = json.load(f)
            else:
                output_data = {"workflow_id": self.workflow_id, "outputs": []}
                
            # Add new output
            output_data["outputs"].append(output_info)
            output_data["last_update"] = output_info["timestamp"]
            
            # Save to file
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            self.logger.error(f"ä¿å­˜è¾“å‡ºæ›´æ–°å¤±è´¥: {e}")
            
    async def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        try:
            self.logger.info("ğŸ›‘ åœæ­¢è¶…å®æ—¶ç›‘æ§")
            self.monitoring = False
            
            # Stop system
            if self.system:
                await self.system.stop()
                
            # Generate final report
            await self._generate_final_report()
            
        except Exception as e:
            self.logger.error(f"åœæ­¢ç›‘æ§å¤±è´¥: {e}")
            
    async def _generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        try:
            report = {
                "workflow_id": self.workflow_id,
                "start_time": self.start_time,
                "end_time": time.time(),
                "duration": time.time() - self.start_time if self.start_time else 0,
                "total_status_updates": len(self.status_history),
                "total_file_changes": len(self.file_changes),
                "final_status": self.last_status,
                "status_history": self.status_history[-20:],  # Last 20 updates
                "file_changes": self.file_changes[-50:]  # Last 50 changes
            }
            
            report_file = os.path.join(self.output_dir, f"ultra_report_{self.workflow_id}.json")
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
                
            self.logger.info(f"ğŸ“Š è¶…å®æ—¶ç›‘æ§æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šå¤±è´¥: {e}")

async def main():
    """ä¸»å‡½æ•°"""
    try:
        # å®šä¹‰ä¸“åˆ©ä¸»é¢˜
        topic = "åŸºäºæ™ºèƒ½åˆ†å±‚æ¨ç†çš„å¤šå‚æ•°å·¥å…·è‡ªé€‚åº”è°ƒç”¨ç³»ç»Ÿ"
        description = "ä¸€ç§é€šè¿‡æ™ºèƒ½åˆ†å±‚æ¨ç†æŠ€æœ¯å®ç°å¤šå‚æ•°å·¥å…·è‡ªé€‚åº”è°ƒç”¨çš„ç³»ç»Ÿï¼Œèƒ½å¤Ÿæ ¹æ®ä¸Šä¸‹æ–‡å’Œç”¨æˆ·æ„å›¾è‡ªåŠ¨æ¨æ–­å·¥å…·å‚æ•°ï¼Œæé«˜å¤§è¯­è¨€æ¨¡å‹è°ƒç”¨å¤æ‚å·¥å…·çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚"
        
        print("=" * 80)
        print("ğŸš€ å¯åŠ¨è¶…å®æ—¶å·¥ä½œæµç›‘æ§ç³»ç»Ÿ")
        print("=" * 80)
        print(f"ä¸»é¢˜: {topic}")
        print(f"æè¿°: {description}")
        print("=" * 80)
        print("ğŸ“Š ç›‘æ§é—´éš”: çŠ¶æ€æ£€æŸ¥ 1ç§’ | èµ„æºæ£€æŸ¥ 3ç§’ | æ–‡ä»¶æ£€æŸ¥ 0.5ç§’")
        print("=" * 80)
        
        # åˆ›å»ºç›‘æ§å™¨
        monitor = UltraRealTimeMonitor()
        
        # å¯åŠ¨ç›‘æ§
        await monitor.start_monitoring(topic, description)
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨åœæ­¢ç›‘æ§...")
        if 'monitor' in locals():
            await monitor.stop_monitoring()
    except Exception as e:
        print(f"âŒ ç›‘æ§ç³»ç»Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())